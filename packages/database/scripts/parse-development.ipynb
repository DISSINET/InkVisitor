{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Parse development book\n",
    "\n",
    "### Notebook purpose\n",
    "This notebook is development space for python parse.ts replacement and upgrade.\n",
    "It reads specified google sheets and output actants.json file, which can be imported to inkVisitor RethinkDB.py\n",
    "\n",
    "The aggreated doc for dev: https://docs.google.com/document/d/1ga6R_9TWAQlXE9XqPE_qZ2S8S8aVW2A7n8SuYpSXnoI/edit#heading=h.q9ntf0ofam2u\n",
    "The holy schema: https://app.diagrams.net/#G1bKvqEKr6JzPWryVg-vYudQy_4KvuzHS9\n",
    "\n",
    "### JSon schemas for the actants\n",
    " * are created from *.ts files through typescript-jschema commandline utility\n",
    " * the schemas are trasformed as python Classes through warlock library\n",
    "\n",
    "\n",
    "### Principles of the parsing operations\n",
    " * There are google sheet dataset, which need to be transformed to json format according to inkVisior datamodel\n",
    " * inkVisitor holds datamodel in the code, the typescript classes (see /shared/types...), or the holy schema\n",
    " * DZ created in input table parsing instructions\n",
    "   * the first 5 rows contain instructions\n",
    "   * if the keywords contain ? character, they are ignored (it is work in progress from DZ)\n",
    "\n",
    "#### Parsing instructions\n",
    " * explicit set of keywords\n",
    "   * discard\n",
    "     * ...\n",
    "   * inside\n",
    "     * the value in the column should be straitforwadly made part of the entity object, the exists update_generic method or update_fieldName mehthods for fields with custom but generally applicable logic\n",
    "   * propvalue\n",
    "     * for making so called\n",
    "          * statement or property statment (old A0093 has relation)\n",
    "     * it sits in the entity \"props\" attribute, it has IProp class\n",
    "   * special\n",
    "     * fully custom method for the parsing behavior\n",
    "     *\n",
    "\n",
    "### Parsing process\n",
    " 1. Prepare data model entities classes (from typescript inkvisitor classes -> json schema ->  python classes)\n",
    " 2. Load and wrangle all input data\n",
    " 3. Process headers of the tables for parsing instructions\n",
    " 4. Process tables row by row, column by column\n",
    " 5. Save as json file (which can be imported to the inkVisitor RethinkDB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# DEV Updates : November\n",
    " - TODO updated model for relations (attribute order)\n",
    " - TODO synonymic relations parsed to \"cloud\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Dev updates:  October 2\n",
    "- DONE change event-territories generation,  genre is relation.classification, not metaprop\n",
    "- TODO prepare for \"partial parsing\" and fixed guids for \"done\" tables\n",
    "- DONE updated users\n",
    "- DONE change in Manuscript parsing, For each non-empty, non-NA value: check whether the script hasn't already generated an L entity with the same label.\n",
    "IF NOT: (1) Generate a new L entity with label = value in this col., status = \"approved\", entity logical type = \"definite\", label language = value in the next col. (repository_label_language); (2) append to this L entity a RelationType.Classification leading to C2646 \"archive or library\". (3) Go to No. 4.\n",
    "IF YES, go to No. 4 and use that L.\n",
    "(4) Under the O entity representing the row (i.e. the physical manuscript), add a metaprop which will relate this O to this L entity (repository) through the relation: O - (has) - C2645 \"repository\" - L in this col.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dev updates:  October\n",
    " - DONE u action a conceptu se neparsuji relations ?\n",
    " - DONE revize skriptu vzhledem k september bordelu\n",
    " - DONE events parsing\n",
    " - DONE 21.09. 2022, R table has received a new parsable column “components_id”. Its notation follows our agreed-upon notation, so it should not pose any problem, but I rather signal this new col. to you. It is for composite Rs, which need to be separated into several identities with different URIs (typically two volumes of an edition)\n",
    "  - DONE The instruction here and here for generating creation events for Territories and manuscript Objects has changed, since what was the Property C0565 class is now the RelationType Classification. Please take this change into account, tx.\n",
    " - DONE territories generation\n",
    "  - light yellow: metadata of Subterritory capturing the lowest-level document recording the Event (not of Event described by the row). Tomáš H.: (1) Please generate these lowest-level T entities following Adam’s script for parsing Robert’s Sellan coding sheet IDs into a hierarchical T structure and put them under their proper root T through legacy IDs: first element of ID = root T ID, last element = lowest-level Territory whose label follows in the next col., middle elements = folders in between root and lowest child level. E.g. here, in Guglielmites, there is root, i.e. the whole Guglielmites process of Beltramo Salvagno; then parts 1-4; and underneath them, the individual documents. When creating these lowest-level Subterritories, use this attributes: Entity type = T; T label and label_language defined in the two respective cols. which follow; status = approved. Other metadata set as metaprops in the following yellow cols. (2) Generate the middle entities holding the hierarchy, name them by the chunk ID (in this case, this will be four Ts named “1”, “2”, “3”, and “4”), and relate the lowest-level entity to them as their parents, and relate those middle entities to the root T as their parent in turn; attributes of these middle entities (folders 1-4 of the records): label = number [1,2,3,4] depending on the chunk; legacy id = T107-[chunk number], label language = Latin, status = approved. (3) Relate the lowest-level entity to the Event described by the row by the following Metaproperty: T107-1-01 - [has] - C2286 represented event - E0001 (middle-level entities will have no such relation now, only lowest-level documents).\n",
    "\n",
    "\n",
    "# Dev updates:  September\n",
    " - adding new users\n",
    "\n",
    "# Dev updates:  August\n",
    " - new instruction with proptype_2nd in column\n",
    "    - before we got the type fixed and had just instruction propvalue_2nd (used in territories for manuscript witness)\n",
    " - new instruction \"hook-inside\" for insides in newly created entities (see in gugli locations special:modern_name)\n",
    " - changes :  some properties 'id' changed to 'specificId'\n",
    "   - IProp {... 'value':{'entityId' and 'type':{'entityId'\n",
    "   - IStatement {  'data':{ 'territory' {'territoryId'\n",
    "   - ITerritory { 'data': {'parent': {'territoryId': \"\"\n",
    "- bigger TODO relations\n",
    "\n",
    "# Dev updates: April\n",
    "\n",
    "## Concepts\n",
    "  * DONE wordnet_synset_id\n",
    "    * wordnet_id bounded to resource object R0067\n",
    "\n",
    "## Texts\n",
    " * DONE special_creation_event_id\n",
    "   * instructions:\n",
    "     * Create entities in this col. as new E entities, with (1) the value here as legacy_id, (2) assign (as usual) a new \"hash\" ID from the db, (3) label of this E: see next col., (4) logical type \"definite\" (default), (5) label language \"English\", (6) status \"approved\", and (7) attach to any of those Es the metaprop \"(has) - C0565 \"class\" - C2642 \"creation\" (to instantiate the event to its event type = event class).\n",
    "   * creation_event_label in standalone field\n",
    "     * this will trigger the operation\n",
    "   * lot of proptype and propvalue with non-standard schema: new branch of interpreting\n",
    "\n",
    "## Manuscripts\n",
    " * special_repository_label\n",
    "   * i:\n",
    "     * For each non-empty, non-NA, non-NS row: (1) generate L entity with label = value in this col., status = \"approved\", entity logical type = \"definite\", label language = value in the next col. (repository_label_language); (2) append to this L entity a metaprop (has) - C0565 \"class\" - C2646 \"archive or library\", and (3) under the O entity representing the row (i.e. the physical manuscript), add a metaprop which will relate this O to this L entity (repository) through the relation: O - (has) - C2645 \"repository\" - L in this col.\n",
    "   * repository_label_language\n",
    "     * i:\n",
    "       * Use this value as label language value of the repository L entity.\n",
    " * DONE creation_event_id\n",
    "   * creation_event_label\n",
    "   * i:\n",
    "     * Create entities in this col. as new E entities, with (1) the value here as legacy_id, (2) assign (as usual) a new \"hash\" ID from the db, (3) label of this E: see next col., (4) logical type \"definite\" (default), (5) label language \"English\", (6) status \"approved\", and (7) attach to any of those Es the metaprop \"(has) - C0565 \"class\" - C2642 \"creation\" (to instantiate the event to its event type = event class).\n",
    " * reproduction_online_url\n",
    "   * instructions:\n",
    "     * If non-empty, non-NA, (1) generate an R entity with label \"Reproduction of \" + label of the MS (i.e. value in the B column, status = \"approved\", label-language = \"English\", url = the URL sitting under the hyperlink value in this cell, and (2) add metaprop to the O entity represented by this row: O - (has) - C1199 \"digital reproduction\" - the R entity here generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tables = [\"texts\", \"manuscripts\", \"resources\", \"actions\" , \"concepts\"]\n",
    "\n",
    "#                   sheet_name,  gdrive code, header_in_row, columns which need suffix based on a case name\n",
    "input_sheets = {\n",
    "    \"texts\" : (\"Texts\",\"13eVorFf7J9R8YzO7TmJRVLzIIwRJS737r7eFbH1boyE\", 5), #https://docs.google.com/spreadsheets/d/13eVorFf7J9R8YzO7TmJRVLzIIwRJS737r7eFbH1boyE/edit#gid=2056508047\n",
    "    \"manuscripts\" : (\"Manuscripts\", \"13eVorFf7J9R8YzO7TmJRVLzIIwRJS737r7eFbH1boyE\", 5),\n",
    "    \"resources\" : (\"Resources\", \"13eVorFf7J9R8YzO7TmJRVLzIIwRJS737r7eFbH1boyE\", 5),\n",
    "    \"actions\" :  (\"Statements\",\"1vzY6opQeR9hZVW6fmuZu2sgy_izF8vqGGhBQDxqT_eQ\", 5), # https://docs.google.com/spreadsheets/d/1vzY6opQeR9hZVW6fmuZu2sgy_izF8vqGGhBQDxqT_eQ/edit#gid=0\n",
    "    \"concepts\" : (\"Concepts\",\"1nSqnN6cjtdWK-y6iKZlJv4iGdhgtqkRPus8StVgExP4\", 5), # https://docs.google.com/spreadsheets/d/1nSqnN6cjtdWK-y6iKZlJv4iGdhgtqkRPus8StVgExP4/edit#gid=0,\n",
    "    \"R0006_persons\" : (\"Persons\",\"1UtgKvjr91egdgOQMP8WxsRxgGDT9FC92qBG3sKBiYvk\", 5, ['id','residence_id','social_relation1_value_id','social_relation2_value_id','social_relation3_value_id']), # GUGLIELMITES Persons, https://docs.google.com/spreadsheets/d/1UtgKvjr91egdgOQMP8WxsRxgGDT9FC92qBG3sKBiYvk/edit#gid=0,\n",
    "    \"R0007_locations\" : (\"Locations\",\"1jImuYCTu-QrZc7oCT1Vz01SLu5sxYRo4xzJzlF2J8Bs\", 5, ['id','locationrelation1_value_id','superordinate_location_id']) ,# GULIELMITES Locations, https://docs.google.com/spreadsheets/d/1jImuYCTu-QrZc7oCT1Vz01SLu5sxYRo4xzJzlF2J8Bs/edit#gid=0,\n",
    "    \"R0008_events\" : (\"Events\",\"1mLevawnoZVLQv6qINaSfiyi1NNlfJzTZevinRSSQVPo\", 5, ['id','spatial_ref_value_id','participant_id','inquisitor_id','notary_id','witness_assessor_id','accuser_id','accused_id','mentioned_id','denied_id','marked_as_incriminated_id','temporal_ref_1_value_id','temporal_ref_2_value_id']),\n",
    "    # ORIGINAl https://docs.google.com/spreadsheets/d/1mLevawnoZVLQv6qINaSfiyi1NNlfJzTZevinRSSQVPo/edit#gid=0,\n",
    "    \"R0075_persons\" : (\"Persons\",\"1Tyi4DYnHDeTs8tuz9rxN3wyq1LqLWWp_ZPgOimQoxf8\", 5, ['id','origin_or_residence_id','residence_id','relations_1_value_id','relations_2_value_id','relations_3_value_id','relations_4_value_id','relations_5_value_id','relations_6_value_id','relations_7_value_id','relations_8_value_id','relations_9_value_id','relations_10_value_id','relations_11_value_id','relations_12_value_id','relations_13_value_id','origin_id','occupation_id','office_id']), # Castellario Persons, https://docs.google.com/spreadsheets/u/1/d/1Tyi4DYnHDeTs8tuz9rxN3wyq1LqLWWp_ZPgOimQoxf8/edit#gid=0\n",
    "    \"R0035_locations\" : (\"Locations\",\"1VwUDnAMJ8mWvL-54K-OnZmguB-rALnJjtGG2IGNBqIU\", 5, ['id','locationrelation1_value_id','locationrelation2_value_id']) ,# Piedmont Castellario Locations, https://docs.google.com/spreadsheets/d/1VwUDnAMJ8mWvL-54K-OnZmguB-rALnJjtGG2IGNBqIU/edit#gid=0\n",
    "    \"R0083_events\" : (\"Events\",\"1M5uhJsbjUCxWP_avYQM0fr1Hm0GXVmYuDvqSoEuUAbs\", 5, ['id','spatial_ref_value_id','participant_id','inquisitor_id','notary_id','witness_assessor_id','accuser_id','accused_id','mentioned_id','denied_id','temporal_ref_1_value_id','temporal_ref_2_value_id'])\n",
    "    # Castellario Incriminations https://docs.google.com/spreadsheets/u/1/d/1M5uhJsbjUCxWP_avYQM0fr1Hm0GXVmYuDvqSoEuUAbs/edit#gid=0\n",
    "}\n",
    "\n",
    "# resource IDs\n",
    "# R0006 Guglielmites Persons\n",
    "# R0007\tGuglielmites Locations\n",
    "# R0008\tGuglielmites Events\n",
    "\n",
    "# R0035\tPiedmont Locations Castellario\n",
    "# R0075\tPersons Castellario\n",
    "# R0083 Castellario Incriminations\n",
    "\n",
    "table_to_entity = {\n",
    "    \"concepts\" : \"IConcept\",\n",
    "    \"resources\" : \"IResource\",\n",
    "    \"texts\" : \"ITerritory\",\n",
    "    \"manuscripts\" : \"IObject\",\n",
    "    \"actions\" :  \"IAction\",\n",
    "    \"R0006_persons\" : \"IPerson\",\n",
    "    \"R0007_locations\": \"ILocation\",\n",
    "    \"R0008_events\": \"IEvent\",\n",
    "    \"R0075_persons\" : \"IPerson\",\n",
    "    \"R0035_locations\": \"ILocation\",\n",
    "    \"R0083_events\": \"IEvent\",\n",
    "}\n",
    "\n",
    "root_sheet_url = \"https://docs.google.com/spreadsheets/d/\"\n",
    "google_api_dotenv_path = \"../env/.env.googleapi\"  # contains google api specs for sheet access with Dator\n",
    "schema_path = '../schemas/' # path for dir with schemas\n",
    "json_schemas = {}  # holder for schemas, so they can be used for jsonschema validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.run(\"python generate-json-schemas.py\", shell=True,capture_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warlock, json\n",
    "from datetime import datetime\n",
    "import time\n",
    "from jsonschema import validate\n",
    "import dissinetpytools.dator as dator\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from shutil import copyfile\n",
    "\n",
    "import uuid\n",
    "\n",
    "def get_uuid_id():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "def is_valid_uuid(val):\n",
    "    try:\n",
    "        uuid.UUID(str(val))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# type hinting\n",
    "from collections.abc import Sequence, Callable\n",
    "from typing import List, Dict, Tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(google_api_dotenv_path) # fills os.environ['GDRIVE_API_CREDENTIALS']\n",
    "d = dator.Dator(loglevel=10, print_log_online=True, cache=True, project_name=\"inkvisitor-import\") # expects 'GDRIVE_API_CREDENTIALS' in the global system variables (os.environ)\n",
    "d.google_authenticate()\n",
    "logger = d.logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all schemas inside and warlock them as globally available classes\n",
    "schema_filenames = os.listdir(schema_path)\n",
    "json_classes = {}\n",
    "for schema in schema_filenames:\n",
    "    name = schema.split(\".schema\")[0]\n",
    "    file_handler = open(schema_path + schema,\"r\")\n",
    "    schema_json = json.load(file_handler)\n",
    "    json_schemas[name] = schema_json\n",
    "    globals()[name] = warlock.model_factory(schema_json)\n",
    "    json_classes[name] = globals()[name]\n",
    "    logger.info(\"Class \" + name + \" available.\")\n",
    "\n",
    "logger.info(f\"There are {len(json_classes.keys())} json classes available ({' '.join(json_classes.keys())}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factory for making entity objects, contains defaults with \"prerequisities\"\n",
    "from datetime import datetime\n",
    "import_note = \"Import batch [development] \" + str(datetime.now())\n",
    "\n",
    "class InkVisitorJSONObjectFactory:\n",
    "\n",
    "    classes = json_classes\n",
    "\n",
    "    json_class_defaults = {\n",
    "        'IAction':{\n",
    "            'class':'A', 'id':'','legacyId':'', 'label':'', 'language':'', 'detail':'','data':{'entities':{'a1':[],'a2':[],'s':[]},'valencies':{'a1':'','a2':'','s':''}}, 'props':[], 'notes':[], 'status':'1', 'references':[]\n",
    "        },\n",
    "        'IConcept':{\n",
    "            'class':'C', 'id':'', 'legacyId':'','label':'', 'language':'', 'detail':'','data':{}, 'props':[], 'notes':[], 'status':'1', 'references':[]\n",
    "        },\n",
    "        'IValue':{\n",
    "            'class':'V', 'id':'', 'label':'', 'language':'', 'detail':'','data':{'logicalType':'1'}, 'props':[], 'notes':[], 'status':'1', 'references':[]\n",
    "        },\n",
    "        'IProp':{\n",
    "            'bundleEnd':False,'bundleStart':False, 'certainty':'0', 'children':[], 'elvl':'3', 'id':'', 'logic':'1', 'mood':['1'], 'moodvariant':'1', 'bundleOperator':'a', 'type': {'entityId':'','elvl':'3','logic':'1','partitivity':'1','virtuality':'1'},'value':{'entityId':'','elvl':'3', 'logic':'1', 'partitivity':'1', 'virtuality':'1'}\n",
    "        },\n",
    "        'IResource':{\n",
    "             'class':'R', 'id':'', 'label':'', 'language':'', 'detail':'','data':{'url':'','partValueBaseURL':'','partValueLabel':''}, 'props':[], 'notes':[], 'status':'1', 'references':[]\n",
    "        },\n",
    "        'Relation.IIdentification':{\n",
    "            'id':'', 'type':'IDE', 'certainty':'0','entityIds': ['','']\n",
    "        },\n",
    "        'Relation.IClassification':{\n",
    "            'id':'', 'type':'CLA','entityIds': ['',''],\"order\":1\n",
    "        },\n",
    "        'Relation.IImplication':{\n",
    "            'id':'', 'type':'IMP','entityIds': ['',''], \"order\":1\n",
    "        },\n",
    "        'Relation.IHolonym':{\n",
    "            'id':'', 'type':'HOL','entityIds': ['',''],\"order\":1\n",
    "        },\n",
    "        'Relation.IRelated':{\n",
    "            'id':'', 'type':'REL','entityIds': ['',''], \"order\":1\n",
    "        },\n",
    "        'Relation.IActionEventEquivalent':{\n",
    "            'id':'', 'type':'AEE','entityIds': ['','']\n",
    "        },\n",
    "        'Relation.ISubjectActant1Reciprocal':{\n",
    "            'id':'', 'type':'SAR','entityIds': ['','']\n",
    "        },\n",
    "        'Relation.IPropertyReciprocal':{\n",
    "            'id':'', 'type':'PRR','entityIds': ['','']\n",
    "        },\n",
    "        'Relation.IAntonym':{\n",
    "            'id':'', 'type':'ANT','entityIds': ['',''], \"order\":1\n",
    "        },\n",
    "        'Relation.ISynonym':{\n",
    "            'id':'', 'type':'SYN','entityIds': ['','']\n",
    "        },\n",
    "        'Relation.ISuperordinateLocation':{\n",
    "            'id':'', 'type':'SOL','entityIds': ['',''], \"order\":1\n",
    "        },\n",
    "        'Relation.ISuperclass':{\n",
    "            'id':'', 'type':'SCL','entityIds': ['',''], \"order\":1\n",
    "        },\n",
    "        'Relation.ISubjectSemantics':{\n",
    "            'id':'', 'type':'SUS','entityIds': ['',''], \"order\":1\n",
    "        },\n",
    "        'Relation.IActant1Semantics':{\n",
    "            'id':'', 'type':'A1S','entityIds': ['',''], \"order\":1\n",
    "        },\n",
    "        'Relation.IActant2Semantics':{\n",
    "            'id':'', 'type':'A2S','entityIds': ['',''], \"order\":1\n",
    "        },\n",
    "        'IObject':{\n",
    "             'class':'O', 'id':'', 'label':'', 'language':'', 'detail':'','data':{'logicalType':'1'}, 'props':[], 'notes':[], 'status':'1', 'references':[]\n",
    "        },\n",
    "        'IStatement':{\n",
    "             'class':'S', 'id':'', 'label':'', 'language':'', 'detail':'','data':{'actants':[], 'actions':[], 'tags':[],'territory': {'territoryId':'','order':0}, 'text':''}, 'props':[], 'notes':[], 'status':'1','references':[]\n",
    "        },\n",
    "        'ITerritory':{\n",
    "            'class':'T', 'id':'', 'legacyId':'','label':'', 'language':'', 'detail':'','data':{'parent':{ \"territoryId\": \"T0\", \"order\": 0 }}, 'props':[], 'notes':[], 'status':'1', 'references':[]\n",
    "        },\n",
    "        'ILocation':{\n",
    "            'class':'L', 'id':'', 'label':'', 'language':'', 'detail':'','data':{'logicalType':'1'}, 'props':[], 'notes':[], 'status':'1', 'references':[]\n",
    "        },\n",
    "        'IEvent':{\n",
    "            'class':'E', 'id':'', 'label':'', 'language':'', 'detail':'','data':{'logicalType':'1'}, 'props':[], 'notes':[], 'status':'1', 'references':[]\n",
    "        },\n",
    "        'IReference':{\n",
    "           'id':'','resource':'','value':''\n",
    "        },\n",
    "        'IAudit':{\n",
    "           'id':'','entityId':'','user':'','date':'','changes':{}\n",
    "        },\n",
    "        'IPerson':{\n",
    "            'class':'P', 'id':'', 'label':'', 'language':'', 'detail':'','data':{'logicalType':'1'}, 'props':[], 'notes':[], 'status':'1', 'references':[]\n",
    "        },\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        for key, item in type(self).json_class_defaults.items():\n",
    "            if 'notes' in item and  len(item['notes']) == 0:\n",
    "                item['notes'].append(import_note)\n",
    "\n",
    "    def make(self, entity_name, override_object=None):\n",
    "        if override_object is None:\n",
    "            override_object = {}\n",
    "        object = type(self).json_class_defaults[entity_name]\n",
    "        object.update(override_object)\n",
    "        return type(self).classes[entity_name](deepcopy(object))\n",
    "\n",
    "    def validate_defaults(self):\n",
    "        for e in self.json_class_defaults:\n",
    "            d.logger.info(f\"Trying to validate class {e}.\")\n",
    "            test = self.make(e, self.json_class_defaults[e])\n",
    "            d.logger.info(f\"Class {e} validated.\")\n",
    "\n",
    "\n",
    "IOF = InkVisitorJSONObjectFactory()\n",
    "IOF.validate_defaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialisation of input data tables\n",
    "tables = {}\n",
    "header_infos = {}\n",
    "entity_ids = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load input datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial load\n",
    "# partial_load = ['R0075_persons', 'R0035_locations','R0083_events']\n",
    "# partial_load = ['R0035_locations']\n",
    "partial_load = []\n",
    "\n",
    "# empty value unifier\n",
    "def unify_empty_value(df: pd.DataFrame, empty_values=None, unified_empty_value =''):\n",
    "    if empty_values is None:\n",
    "        empty_values = [\"#N/A\", \"#VALUE!\",'NA', 'NS'] # 'NA', 'NS',\n",
    "    for naner in empty_values:\n",
    "        df = df.replace(naner,unified_empty_value, regex=True)\n",
    "    df. fillna(unified_empty_value, inplace=True)\n",
    "    return df\n",
    "\n",
    "# define cross-suffixes\n",
    "casearea_to_entity_suffix = {\n",
    "    \"R0006\" : {\n",
    "        \"P\" : \"R0006\",\n",
    "        \"L\" : \"R0007\",\n",
    "        \"E\" : \"R0008\"\n",
    "    },\n",
    "    \"R0007\" : {\n",
    "        \"P\" : \"R0006\",\n",
    "        \"L\" : \"R0007\",\n",
    "        \"E\" : \"R0008\"\n",
    "    },\n",
    "    \"R0008\" : {\n",
    "        \"P\" : \"R0006\",\n",
    "        \"L\" : \"R0007\",\n",
    "        \"E\" : \"R0008\"\n",
    "    },\n",
    "    \"R0075\" : {\n",
    "        \"P\" : \"R0075\",\n",
    "        \"L\" : \"R0035\",\n",
    "        \"E\" : \"R0083\"\n",
    "    },\n",
    "    \"R0035\" : {\n",
    "        \"P\" : \"R0075\",\n",
    "        \"L\" : \"R0035\",\n",
    "        \"E\" : \"R0083\"\n",
    "    },\n",
    "    \"R0083\" : {\n",
    "        \"P\" : \"R0075\",\n",
    "        \"L\" : \"R0035\",\n",
    "        \"E\" : \"R0083\"\n",
    "    },\n",
    "}\n",
    "\n",
    "def get_suffix_based_on_case_and_entity_type(string, case_suffix):\n",
    "    # if valid entity type\n",
    "    allowed_strict_entities = ['P','L','E','B']\n",
    "    if any(string.startswith(c)for c in allowed_strict_entities):\n",
    "        fl = string[0]\n",
    "        return \"_\"+casearea_to_entity_suffix[case_suffix.replace(\"_\",\"\")][fl]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def apply_suffix(string, case_suffix):\n",
    "    if string in ['NS','NA']:\n",
    "        return string\n",
    "    mod_string = \"\"\n",
    "    if \"#\" in string:\n",
    "        strings = string.split(\"#\")\n",
    "        for s in strings:\n",
    "            mod_string += \" #\"+ s.strip() + get_suffix_based_on_case_and_entity_type(s.strip(), case_suffix)\n",
    "        mod_string = mod_string[2:]\n",
    "    else:\n",
    "        mod_string = string + get_suffix_based_on_case_and_entity_type(string, case_suffix)\n",
    "    return mod_string\n",
    "\n",
    "\n",
    "# load all input tables\n",
    "load_sheets = {}\n",
    "if len(partial_load) > 0:\n",
    "    logger.info(f\"Making PARTIAL LOAD with {partial_load}\")\n",
    "    for item in partial_load:\n",
    "        load_sheets[item] = input_sheets[item]\n",
    "else:\n",
    "    load_sheets = input_sheets\n",
    "\n",
    "for key, sheet in load_sheets.items():\n",
    "    logger.info(f\"Calling for {key} with sheet_name {sheet[0]}.\")\n",
    "    tables[key], header_infos[key] = d.load_df_from_gsheet(key,root_sheet_url + sheet[1], sheet[0], fromCache=False, header_in_row=sheet[2], clean=True, fillna=True, cleanByColumn=\"label\", parse_hyperlink_formulas=True, numerize=False)\n",
    "    tables[key] = unify_empty_value(tables[key])\n",
    "    header_infos[key] = unify_empty_value(header_infos[key])\n",
    "\n",
    "    # clean brackets\n",
    "    tables[key] = tables[key].replace(\"\\[\",\"\",  regex=True)\n",
    "    tables[key] = tables[key].replace(\"\\]\",\"\",  regex=True)\n",
    "\n",
    "\n",
    "    # case study dataset legacyId needs a suffix\n",
    "    if \"_\" in key:\n",
    "        columns = sheet[3]\n",
    "        suffix = \"_\" + key.split(\"_\")[0]\n",
    "        for c in columns:\n",
    "            # tables[key][c] = tables[key][c] + suffix\n",
    "            logger.info(f\"Using {key} and {c}.\")\n",
    "            tables[key][c]  = tables[key][c].apply(lambda item: apply_suffix(item,suffix) if len(str(item)) > 0 else item)\n",
    "\n",
    "            #tables[key].loc[~tables[key][c].isna(), \"c\"] = tables[key][c] + \"_\" + suffix\n",
    "            # df.loc[df[\"gender\"] == \"male\", \"gender\"] = 1\n",
    "            logger.info(f\"{key} For columns {c} the suffix from {suffix} set was glued.\")\n",
    "\n",
    "    # code for legacyId copy and uuid creation\n",
    "    # tables[key]['legacyId'] = tables[key]['id'].copy()\n",
    "    tables[key].insert(2, 'legacyId', tables[key]['id'].copy())\n",
    "\n",
    "    # inform instructive header about the new column and what to do with it\n",
    "    header_infos[key]['legacyId'] = \"\"\n",
    "    header_infos[key].at[3,'legacyId'] = \"inside\"\n",
    "    tables[key]['id'] = tables[key].apply(lambda x: get_uuid_id(), axis=1)  # generate unique id for each row\n",
    "    # make id dictionaries (it is much faster to search for keys there in legacyId>id retrievals)\n",
    "    ed = tables[key][[\"legacyId\",\"id\"]].set_index(\"legacyId\")\n",
    "\n",
    "    if \"_\" in key:   # legacyIds are case-specified, i.e. all locations can be together in one table\n",
    "        key = key.split(\"_\")[1]\n",
    "\n",
    "    if key in  entity_ids:\n",
    "        entity_ids[key] = {**entity_ids[key],**ed[\"id\"].to_dict()}\n",
    "    else:\n",
    "        entity_ids[key] = ed[\"id\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making empty table of values\n",
    "tables['values'] = pd.DataFrame(columns=['id','value','origin'])\n",
    "tables['locations'] = pd.DataFrame(columns=['id','value','origin','legacyId'])\n",
    "tables['events'] = pd.DataFrame(columns=['id','value','origin','legacyId'])\n",
    "tables['props'] = pd.DataFrame(columns=['id','type','type_id','value','value_id','original_field','origin','entityId','legacyId'])\n",
    "tables['relations'] = pd.DataFrame(columns=['id','type','logic','certainty','entityIds','origin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tables[\"R0008_events\"] = tables[\"R0008_events\"].replace(\"\\[\",\"\",  regex=True)\n",
    "#tables[\"R0008_events\"] = tables[\"R0008_events\"].replace(\"\\]\",\"\",  regex=True)\n",
    "\n",
    "df = tables[\"concepts\"]\n",
    "df[df['label']=='social category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['label']=='occupation']['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_ids['locations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual corrections in the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Responsible editors\n",
    "\n",
    "#### Data\n",
    "Only concepts have \"editors\" filled.\n",
    "\n",
    "#### Specification\n",
    "\n",
    "Only entities have audits, i.e. the prop object, the ref object don't. (Could be wrong?)\n",
    "\n",
    "**Rationale:**  Each entity will have 2 audit creation records. One with the user \"import\", second with the user \"responsible editor.\n",
    "\n",
    "Implementaions possiblities:\n",
    "  - make the audits during the parsing : the way\n",
    "  - make the audits afer the parsing is done (for secondary created entities, I will not have the context, and will be problem to find a responsible editor\n",
    "  -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables['R0075_persons']['editor'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the names in the input tables  vs their user ids\n",
    "# the tuser are imported through the user.json file\n",
    "\n",
    "#David Zbíral\tdavid.zbiral@mail.muni.cz\n",
    "#Robert Shaw\trobert.shaw@mail.muni.cz\n",
    "#Jan Král\tkral.jan@mail.muni.cz\n",
    "#Reima Välimäki\treima.valimaki@gmail.com\n",
    "#Lidia Hinz-Wieczorek\tlidhin93@gmail.com\n",
    "#Davor Salihović\tdavor.salihovic@mail.muni.cz\n",
    "#Katia Riccardo\tkatia.riccardo@mail.muni.cz\n",
    "#Larissa de Freitas Lyth\tlarissa.lyth@mail.muni.cz\n",
    "#Larissa de Freitas Lyth\n",
    "\n",
    "editors = {\"import\":\"0\", \"David Zbíral\":\"100\", \"Robert Shaw\":\"101\", \"Davor Salihović\":\"102\", \"Katia Riccardo\":\"103\", \"Jan Král\":\"104\", \"Reima Välimäki\":\"105\",\"Lidia Hinz-Wieczorek\":\"106\", \"Larissa de Freitas Lyth\":\"107\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Declaration of controlling classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_entities = []\n",
    "relation_records = []\n",
    "audits = []\n",
    "\n",
    "# for controlling entity and mapping of its fields\n",
    "class EntityMapper:\n",
    "\n",
    "    # simple inside values mapping from input_values in gsheets to inkVisitor enums\n",
    "    # field: { FROM  : TO }\n",
    "    enum_mapper = {'language': {\"English\":\"eng\",\"Latin\":\"lat\",\"Occitan\":\"oci\",\"Middle English\":\"enm\",\"Czech\":\"ces\",\"Italian\":\"ita\",\"French\":\"fra\",\"German\":\"deu\"},\"status\":{\"approved\":\"1\",\"pending\":\"0\",\"discouraged\":\"2\",\"warning\":\"3\"}, \"entity_logical_type\":{'definite' : \"1\",\n",
    "  'indefinite' : \"2\",   'hypothetical' : \"3\",  'generic' : \"4\"}}\n",
    "\n",
    "    # status  Pending = \"0\",   Approved = \"1\",  Discouraged = \"2\",  Warning = \"3\",\n",
    "    valid_entity_classes = ['A','C','E','O','B','R','T','P','G','S','L','NULL']\n",
    "\n",
    "\n",
    "    IOF = InkVisitorJSONObjectFactory()\n",
    "\n",
    "    def __init__(self, entity_type, data_row, logger = d.logger):\n",
    "        self.entity =  type(self).IOF.make(entity_type)\n",
    "        self.entity_type = entity_type\n",
    "        self.logger = logger\n",
    "        self.debug = True\n",
    "        self.data_row = data_row\n",
    "\n",
    "    def make_alive(self, legacyId=\"\",label=\"\", label_language=\"\", register_id_where=\"\"):\n",
    "\n",
    "        self.update_id('make_alive',get_uuid_id())\n",
    "        self.update_legacyId('make_alive',legacyId)\n",
    "        self.update_label('make_alive',label)\n",
    "        self.update_label_language('make_alive',label_language)\n",
    "\n",
    "        if register_id_where!=\"\" and legacyId!=\"\":\n",
    "            entity_ids[register_id_where][legacyId] = self.entity['id']\n",
    "\n",
    "        #save entity to the pool, which goes to import json file\n",
    "        if pd.DataFrame(additional_entities)[pd.DataFrame(additional_entities)['id']==self.entity['id']].empty:\n",
    "            additional_entities.append(self.entity)\n",
    "            #logger.info(f\"Appending {legacyId} territory {self.entity['id']} to the pool of additional entities.\")\n",
    "        else:\n",
    "            pass\n",
    "            #logger.info(f\"Territory {legacyId} {self.entity['id']} exists in the pool of additional entities.\")\n",
    "\n",
    "\n",
    "    def make_prop_object(self,prop_type_id, prop_value_id):\n",
    "        prop_object = IOF.make('IProp')\n",
    "        prop_object['id'] = get_uuid_id()\n",
    "        prop_object['type']['entityId'] = prop_type_id\n",
    "        prop_object['value']['entityId'] = prop_value_id\n",
    "        return prop_object\n",
    "\n",
    "    def prepare_prop_object_data(self,prop_type, prop_value, origin = \"\"):\n",
    "        prop_type_id = \"\"\n",
    "        prop_value_id = \"\"\n",
    "\n",
    "        # logger.info(f\"Prepare prop object data>  {prop_type}, {prop_value}\")\n",
    "\n",
    "        # allowed entities in input\n",
    "        allowed_strict_entities = ['C','M','A','E','L','R','S','V','O','T','P'] # should be followed by numbers\n",
    "        allowed_free_string_entities = ['~V~']\n",
    "\n",
    "        # checking input_value\n",
    "        if not is_valid_uuid(prop_value):\n",
    "            if any(prop_value.startswith(c)for c in allowed_strict_entities):\n",
    "                # check for numbers\n",
    "                if not prop_value[1:4].isnumeric():\n",
    "                    prop_value = \"~V~\"+prop_value\n",
    "            elif not any(prop_value.startswith(c)for c in allowed_free_string_entities):\n",
    "                prop_value = \"~V~\"+prop_value\n",
    "\n",
    "            # logger.info(f\"Going to search for id for {prop_value}\")\n",
    "            prop_value_id = self.get_entity_id(prop_value, origin=origin)\n",
    "        else:\n",
    "           prop_value_id = prop_value\n",
    "\n",
    "        # TODO this is useless now, because the line above will make ~V~ from everything unknown\n",
    "        assert any(prop_value.startswith(c)  for c in allowed_strict_entities) or any(prop_value.startswith(c)  for c in allowed_free_string_entities) or is_valid_uuid(prop_value), f\"Prop value unknown, C string or V string entity expected, or valid uuid.{prop_value}, {origin}\"\n",
    "\n",
    "        prop_type_id = self.get_entity_id(prop_type, origin=origin)\n",
    "\n",
    "        return prop_type_id, prop_value_id\n",
    "\n",
    "\n",
    "\n",
    "    def make_ref_object(self, ref_resource_id, ref_value_id):\n",
    "        ref_object = IOF.make('IReference')\n",
    "        ref_object['id'] = get_uuid_id()\n",
    "        ref_object['resource'] = ref_resource_id\n",
    "        ref_object['value'] = ref_value_id\n",
    "        return ref_object\n",
    "\n",
    "    def get_entity_id(self,entity_string, origin = \"\"):\n",
    "        id = \"\"\n",
    "        # logger.info(f\"Getting entity string {entity_string} in {origin}\")\n",
    "        try:\n",
    "            if entity_string.startswith(\"C\") and entity_string[1:4].isnumeric():\n",
    "                id = entity_ids[\"concepts\"][entity_string]\n",
    "            elif entity_string.startswith(\"~V~\"):\n",
    "                ventity = self.make_ventity(entity_string, origin=origin)\n",
    "                id = ventity['id']\n",
    "            elif entity_string.startswith(\"M\") and entity_string[1:4].isnumeric():\n",
    "                id = entity_ids[\"manuscripts\"][entity_string]\n",
    "            elif entity_string.startswith(\"A\") and entity_string[1:4].isnumeric():\n",
    "                id = entity_ids[\"actions\"][entity_string]\n",
    "            elif entity_string.startswith(\"R\") and entity_string[1:4].isnumeric():\n",
    "                id = entity_ids[\"resources\"][entity_string]\n",
    "            elif entity_string.startswith(\"T\") and entity_string[1:4].isnumeric():\n",
    "                id = entity_ids[\"texts\"][entity_string]\n",
    "            elif entity_string.startswith(\"O\") and entity_string[1:4].isnumeric():\n",
    "                id = entity_ids[\"objects\"][entity_string]\n",
    "            elif entity_string.startswith(\"P\") and entity_string[1:4].isnumeric():\n",
    "                id = entity_ids[\"persons\"][entity_string]\n",
    "            elif entity_string.startswith(\"L\") and entity_string[1:4].isnumeric():\n",
    "                id = entity_ids[\"locations\"][entity_string]\n",
    "            elif entity_string.startswith(\"E\") and entity_string[1:4].isnumeric():\n",
    "                id = entity_ids[\"events\"][entity_string]\n",
    "            elif entity_string.startswith(\"T\") and entity_string[1:2].isnumeric():\n",
    "                id = entity_ids[\"texts\"][entity_string]\n",
    "\n",
    "            elif is_valid_uuid(entity_string):\n",
    "                id = entity_string\n",
    "\n",
    "        except IndexError as E:\n",
    "            logger.error(f\"Cannot get entity id from entity string {entity_string} in {origin}. {E}\")\n",
    "            raise Exception(f\"Cannot get entity id '{id}' from value  {entity_string}  in {origin}.\")\n",
    "\n",
    "        if id != \"\" and isinstance(id, str):\n",
    "            # logger.info(f\"Got entity id {id} from entity string {entity_string} in {origin}\")\n",
    "            return id\n",
    "        else:\n",
    "            logger.error(f\"Cannot get entity id '{id}' from value {entity_string}  in {origin}.\")\n",
    "            raise Exception(f\"Cannot get entity id '{id}' from value  {entity_string}  in {origin}.\")\n",
    "\n",
    "    def make_ventity(self, value_string, origin=\"\"):\n",
    "        # logger.info(f\"Generating ventity from {value_string}.\")\n",
    "        # generate value entity object...\n",
    "        ventity = IOF.make('IValue')\n",
    "        ventity['id'] = get_uuid_id()\n",
    "        ventity['label'] = value_string.replace(\"~V~\",\"\")\n",
    "\n",
    "        if self.debug:\n",
    "            ventity['notes'].append(origin)\n",
    "\n",
    "        # register ventity\n",
    "        tables['values'] = tables['values'].append({'id':ventity['id'] ,'value':ventity['label'],\"origin\":origin},ignore_index=True )\n",
    "        additional_entities.append(ventity)\n",
    "\n",
    "        # logger.info(f\"Ventity id={ventity['id']} generated.\")\n",
    "\n",
    "        # create audit record\n",
    "        self.create_audit_record(entity_id=ventity['id'], object=ventity)\n",
    "\n",
    "        return ventity\n",
    "\n",
    "    def make_rentity(self, label, url = \"\", origin=\"\"):\n",
    "        # logger.info(f\"Generating rentity from {value_string}.\")\n",
    "        # generate resource entity object...\n",
    "        rentity = IOF.make('IResource')\n",
    "        rentity['id'] = get_uuid_id()\n",
    "        rentity['label'] = label\n",
    "        rentity['data']['url'] = url\n",
    "\n",
    "        # if self.debug:\n",
    "        #    rentity['notes'].append(origin)\n",
    "\n",
    "        # register rentity\n",
    "        tables['resources'] = tables['resources'].append({'id':rentity['id'] ,'label':rentity['label'],\"dissinet_respository_url\":url, \"origin\":origin}, ignore_index=True )\n",
    "        additional_entities.append(rentity)\n",
    "\n",
    "        # logger.info(f\"Rentity id={rentity['id']} generated.\")\n",
    "        return rentity\n",
    "\n",
    "\n",
    "    def make_eentity(self, label, legacyId, url = \"\", origin=\"\"):\n",
    "\n",
    "        eentity = IOF.make('IEvent')\n",
    "        eentity['id'] = get_uuid_id()\n",
    "        eentity['label'] = label\n",
    "        eentity['legacyId'] = legacyId\n",
    "\n",
    "        if self.debug:\n",
    "            eentity['notes'].append(origin)\n",
    "\n",
    "        # register event\n",
    "        tables['events'] = tables['events'].append({'id':eentity['id'] ,'value':eentity['label'],\"origin\":origin, \"legacyId\":legacyId}, ignore_index=True )\n",
    "        additional_entities.append(eentity)\n",
    "\n",
    "        return eentity\n",
    "\n",
    "    def make_lentity(self, label, legacyId=\"\", url = \"\", origin=\"\"):\n",
    "\n",
    "        lentity = IOF.make('ILocation')\n",
    "        lentity['id'] = get_uuid_id()\n",
    "        lentity['label'] = label\n",
    "        lentity['legacyId'] = legacyId\n",
    "\n",
    "        if self.debug:\n",
    "            lentity['notes'].append(origin)\n",
    "\n",
    "        # register lentity\n",
    "        tables['locations'] = tables['locations'].append({'id':lentity['id'] ,'value':lentity['label'],\"origin\":origin,\"legacyId\":legacyId}, ignore_index=True )\n",
    "        additional_entities.append(lentity)\n",
    "\n",
    "        return lentity\n",
    "\n",
    "    def make_tentity(self, label, legacyId=\"\", url = \"\", origin=\"\"):\n",
    "\n",
    "        tentity = IOF.make('ITerritory')\n",
    "        tentity['id'] = get_uuid_id()\n",
    "        tentity['label'] = label\n",
    "        tentity['legacyId'] = legacyId\n",
    "\n",
    "        if self.debug:\n",
    "            tentity['notes'].append(origin)\n",
    "\n",
    "        # register tentity\n",
    "        tables['territories'] = tables['territories'].append({'id':tentity['id'] ,'value':tentity['label'],\"origin\":origin,\"legacyId\":legacyId}, ignore_index=True)\n",
    "        if legacyId != \"\":\n",
    "            entity_ids['texts'][legacyId] = tentity['id']\n",
    "        additional_entities.append(tentity)\n",
    "\n",
    "        return tentity\n",
    "\n",
    "    def make_relation_identity_record(self, id1, id2, certainty=\"\", origin=\"\"):\n",
    "\n",
    "        rir = IOF.make('Relation.IIdentification')\n",
    "        rir['id'] = get_uuid_id()\n",
    "        rir['entityIds'] = [id1,id2]\n",
    "\n",
    "        if certainty != \"\":\n",
    "            rir['certainty'] = certainty\n",
    "\n",
    "        # register relation\n",
    "        tables['relations'] = tables['relations'].append({'id':rir['id'] ,'type':rir['type'], \"certainty\":rir['certainty'],\"entityIds\":rir['entityIds'], \"origin\":origin}, ignore_index=True )\n",
    "        relation_records.append(rir)\n",
    "\n",
    "        return rir\n",
    "\n",
    "    def make_relation_classification_record(self, id1, id2, origin=\"\"):\n",
    "\n",
    "        rcr = IOF.make('Relation.IClassification')\n",
    "        rcr['id'] = get_uuid_id()\n",
    "        rcr['entityIds'] = [id1,id2]\n",
    "\n",
    "        # register relation\n",
    "        tables['relations'] = tables['relations'].append({'id':rcr['id'] ,'type':rcr['type'], \"entityIds\":rcr['entityIds'], \"origin\":origin}, ignore_index=True )\n",
    "        relation_records.append(rcr)\n",
    "\n",
    "        return rcr\n",
    "\n",
    "    def make_relation_record(self, relation_type, ids_list, origin=\"\", order=1):\n",
    "        # genereic relation fc\n",
    "        # BUT synonymic relation have different logic of storage\n",
    "\n",
    "        # a fork to custom function\n",
    "        if relation_type == \"Synonym\":\n",
    "            # rr = self.make_relation_synonymic_record(relation_type, ids_list, origin=\"\")\n",
    "            logger.error(f\"Generic relation record method called, should be handled by special function.\")\n",
    "            return False\n",
    "        else:\n",
    "            rr = IOF.make('Relation.I'+relation_type)\n",
    "            rr['id'] = get_uuid_id()\n",
    "            rr['entityIds'] = ids_list\n",
    "            rr['order'] = order\n",
    "\n",
    "        # logger.info(f\"Making {relation_type} record. ID{rr['id']} EIDS{rr['entityIds']}. Origin: {origin}\")\n",
    "\n",
    "        # register lentity\n",
    "        tables['relations'] = tables['relations'].append({'id':rr['id'] ,'type':rr['type'], \"entityIds\":rr['entityIds'], \"origin\":origin}, ignore_index=True )\n",
    "        relation_records.append(rr)\n",
    "\n",
    "        return rr\n",
    "\n",
    "    def make_relation_synonimic_record(self, relation_type, ids_list, origin=\"\"):\n",
    "\n",
    "        # make new relation object\n",
    "        rr = IOF.make('Relation.I'+relation_type)\n",
    "        rr['id'] = get_uuid_id()\n",
    "        # logger.info(f\"Making synonimic relation record with {ids_list}. To be sorted to {ids_list.sort()}.\")\n",
    "        ids_list.sort()\n",
    "        rr['entityIds'] = ids_list\n",
    "\n",
    "        # check if exists a synonym relation record with the ids\n",
    "        synonymic_relation_exists = False\n",
    "\n",
    "        if synonymic_relation_exists:\n",
    "            pass\n",
    "        else:\n",
    "            # register relation record\n",
    "            tables['relations'] = tables['relations'].append({'id':rr['id'] ,'type':rr['type'], \"entityIds\":rr['entityIds'], \"origin\":origin}, ignore_index=True )\n",
    "            # store relation record\n",
    "            relation_records.append(rr)\n",
    "\n",
    "        return rr\n",
    "\n",
    "    # interprets prop_type (should be always concept or resource) and input_value (should be entity or value string)\n",
    "    # get ids of the prop_type and prop_value (possibly creates and register values object)\n",
    "    # make iProp object\n",
    "    # puts iProp object into the entity props property\n",
    "    def hook_prop_object(self, prop_type, input_value, prop_source=\"\",  origin=\"\", prop_source_id =\"\", field_name=\"\", prop_source_field = \"\"):\n",
    "\n",
    "        # allowed entities in type\n",
    "        assert \"C\" in prop_type[0] or \"R\" in prop_type[0], f\"Prop type unknown, C or R string entity expected? {prop_type}, {origin}\"\n",
    "\n",
    "        prop_type_id, prop_value_id = self.prepare_prop_object_data(prop_type, input_value, origin=origin)\n",
    "        # make IProp object\n",
    "        prop_object = self.make_prop_object(prop_type_id, prop_value_id)\n",
    "\n",
    "        # register prop object\n",
    "        tables['props'] = tables['props'].append({'id':prop_object['id'] , 'type_id':prop_type_id,'value_id':prop_value_id,'type':prop_type,'value':input_value, \"original_field\":field_name, \"origin\":origin, 'entityId':self.entity['id'], 'legacyId':self.entity['legacyId']}, ignore_index=True)\n",
    "\n",
    "        if prop_source_field != \"\":\n",
    "            self.hook_2ndprop_into_props(prop_object,prop_source_field = prop_source_field, origin = origin)\n",
    "        elif prop_source_id !=\"\": # means propvalue_2nd\n",
    "            self.hook_2ndprop_into_props(prop_object,prop_source_id = prop_source_id, origin = origin)\n",
    "        elif prop_source !=\"\": # means propvalue_2nd\n",
    "            self.hook_2ndprop_into_props(prop_object,prop_source = prop_source, origin = origin)\n",
    "\n",
    "        else:\n",
    "            # hook directly into the entity object\n",
    "            self.hook_prop_into_props(prop_object)\n",
    "\n",
    "    def hook_ref_object(self, ref_legacyID, input_value, prop_source=\"\",  origin=\"\"):\n",
    "\n",
    "        # allowed entities in ref_legacyID\n",
    "        assert \"R\" in ref_legacyID, f\"Unknown input, R legacyId expected? {ref_legacyID},{input_value}, {origin}\"\n",
    "\n",
    "        #modify value, so the value object is created\n",
    "        input_value = \"~V~\"+input_value\n",
    "\n",
    "        ref_resource_id = self.get_entity_id(ref_legacyID, origin=origin)\n",
    "        ref_value_id = self.get_entity_id(input_value, origin=origin)\n",
    "\n",
    "        # make IReference object\n",
    "        ref_object = self.make_ref_object(ref_resource_id, ref_value_id)\n",
    "\n",
    "        self.hook_ref_into_refs(ref_object)\n",
    "\n",
    "\n",
    "    def hook_prop_into_props(self,prop_object):\n",
    "        self.entity['props'].append(prop_object)\n",
    "\n",
    "    def hook_ref_into_refs(self,ref_object):\n",
    "        self.entity['references'].append(ref_object)\n",
    "\n",
    "    def hook_2ndprop_into_props(self, prop_object, prop_source = \"\", prop_source_id = \"\", prop_source_field = \"\", origin = \"\"):  # identification by concept id\n",
    "\n",
    "        # recognition based on the prop_source_field\n",
    "        if prop_source_field != \"\":\n",
    "\n",
    "            # count, value in enumerate(values)\n",
    "            for count, po in enumerate(self.entity['props']):\n",
    "                # po['id']\n",
    "                # find whether this id is registered under the prop_source_field within this entity frame\n",
    "                candidate_prop_objects = tables['props'][(tables['props']['legacyId'] ==self.entity['legacyId']) & (tables['props']['original_field'] == prop_source_field)]\n",
    "\n",
    "                #logger.info(f\"2nprop: {len(candidate_prop_objects)}\")\n",
    "                #logger.info(f\"2nprop: {len(candidate_prop_objects)} : {candidate_prop_objects.iloc[0]['id']}\")\n",
    "\n",
    "                if len(candidate_prop_objects) > 0:\n",
    "                    for key, row in candidate_prop_objects.iterrows():\n",
    "\n",
    "                        for  po in self.entity['props']:\n",
    "                            if po['id'] == row['id'] and len(po['children']) == 0:\n",
    "                                # logger.info(f\"Found prop. {po['id']}. Adding the 2ndprop child.\")\n",
    "                                po['children'].append(prop_object)\n",
    "\n",
    "                else:\n",
    "                    logger.error(f\"Cannot find the proper prob object record. {prop_object} at {prop_source_field} in origin '{origin}'.\")\n",
    "        else:\n",
    "            logger.error(f\"Fc hook_2ndprop_into_props run without filled prop_source_field in origin '{origin}'.\")\n",
    "\n",
    "        # recognition based on the type concept of the parent prop object\n",
    "        if prop_source != \"\":\n",
    "            keyId = self.get_entity_id(prop_source)\n",
    "            assert is_valid_uuid(keyId), f\"Cannot recognize entity id from {prop_source}\"\n",
    "\n",
    "            # count, value in enumerate(values)\n",
    "            for count, po in enumerate(self.entity['props']):\n",
    "                if po['type']['entityId'] == keyId and len(po['children']) == 0:  # I am counting on the fact, that if there are relations from multiples, they are processed in the specific right order\n",
    "                   po['children'].append(prop_object)\n",
    "\n",
    "\n",
    "        # recognition based od propobject id\n",
    "        if prop_source_id != \"\":\n",
    "            keyId = prop_source_id\n",
    "            assert is_valid_uuid(keyId), \"Not valid uuid {keyId}\"\n",
    "\n",
    "            for count, po in enumerate(self.entity['props']):\n",
    "                if po['id'] == keyId:  # I am counting on the fact, that if there are relations from multiples, they are processed in the specific right order\n",
    "                   po['children'].append(prop_object)\n",
    "\n",
    "\n",
    "\n",
    "    # method invoker for the INSIDE operation with concrete fields\n",
    "    def update_inside_field(self, field_name, input_value, origin= \"\"):\n",
    "        if input_value != '':\n",
    "\n",
    "            if (\"#\" in input_value or \"~\" in input_value) and field_name!= \"note\" and \"https://docs.\" not in input_value:\n",
    "                self.logger.info(f\"ALERT # or ~ in the input value {input_value}\")\n",
    "\n",
    "            update_operation = \"update_\" + field_name\n",
    "            # update_func = getattr(self, update_operation, self.update_generic)\n",
    "            update_func = getattr(self, update_operation, self.update_generic)\n",
    "            update_func(field_name, input_value, origin)\n",
    "        else:\n",
    "            raise Exception(f\"Trying to update {field_name} with empty input value.\")\n",
    "\n",
    "    #########################################################################################################\n",
    "    # the naming of procedures corresponds to the name of the input_table fields,  used for inside operations\n",
    "\n",
    "    def update_label_language(self, field_name=\"\", input_value=\"\", origin = \"\"):\n",
    "        if input_value in type(self).enum_mapper['language']:\n",
    "            self.entity['language'] = type(self).enum_mapper['language'][input_value]\n",
    "        else:\n",
    "            self.logger.error(f\"Unable to set language in {origin}.\")\n",
    "            self.entity['language'] = input_value # will raise error\n",
    "\n",
    "    def update_status(self, field_name, input_value, origin = \"\"):\n",
    "        if input_value in type(self).enum_mapper['status']:\n",
    "            self.entity['status'] = type(self).enum_mapper['status'][input_value]\n",
    "        else:\n",
    "            self.logger.error(f\"Unable to set status in {origin}.\")\n",
    "            self.entity['status'] = input_value # will raise error\n",
    "\n",
    "    def update_entity_logical_type(self, field_name, input_value, origin = \"\"):\n",
    "        if input_value in type(self).enum_mapper['entity_logical_type']:\n",
    "            self.entity['data']['logicalType'] = type(self).enum_mapper['entity_logical_type'][input_value]\n",
    "        else:\n",
    "            self.logger.error(f\"Unable to set entity logical type in {origin}.\")\n",
    "            self.entity['data']['logicalType'] = input_value # will raise error\n",
    "\n",
    "    def update_note(self, field_name, input_value, origin = \"\"):\n",
    "        #self.logger.info(f\"Updating note with {input_value}.\")\n",
    "        if \"#\" in input_value:\n",
    "            values = [v.strip() for v in input_value.split(\"#\")]\n",
    "            for v in values:\n",
    "                self.entity['notes'].append(v)\n",
    "        else:\n",
    "            self.entity['notes'].append(input_value)\n",
    "\n",
    "    def update_id(self, field_name, input_value, origin = \"\"):\n",
    "        # self.entity['id'] = input_value\n",
    "        self.entity['id'] = input_value\n",
    "\n",
    "    def update_legacyId(self, field_name=\"\", input_value=\"\", origin = \"\"):\n",
    "        # logger.info(f\"Trying to set legacyId {type(input_value)}:'{input_value}' {origin}.\")\n",
    "        self.entity['legacyId'] = input_value\n",
    "\n",
    "    def update_label(self, field_name=\"\", input_value=\"\", origin = \"\"):\n",
    "        self.entity['label'] = input_value\n",
    "\n",
    "    def update_wordnet_lemma_id(self, field_name, input_value, origin = \"\"):\n",
    "        # self.logger.info(f\" wordnet_lemma_id NOT IMPLEMENTED \")\n",
    "        pass\n",
    "\n",
    "    def update_wordnet_synset_id(self, field_name, input_value, origin = \"\"):\n",
    "        # self.logger.info(f\" wordnet_synset_id NOT IMPLEMENTED \")\n",
    "        pass\n",
    "\n",
    "    def update_generic(self, field_name, input_value, origin = \"\"):\n",
    "        self.entity[field_name] = input_value\n",
    "\n",
    "\n",
    "    def create_audit_record(self, entity_id = '', editor_candidate = '', object = {}):\n",
    "\n",
    "        data_row = self.data_row\n",
    "        if entity_id == '':\n",
    "            entity_id = self.entity['id']\n",
    "        if object == {}:\n",
    "            object = self.entity  # should be run at the end of the entity making, to the object is \"full\"\n",
    "\n",
    "        # date =  {\n",
    "        #     \"$reql_type$\": \"TIME\",\n",
    "        #      \"epoch_time\": time.time(),\n",
    "        #      \"timezone\": \"+00:00\"\n",
    "        # }\n",
    "        date =  datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")+\"Z\" #2021-12-05T19:10:15.739Z\n",
    "\n",
    "        if editor_candidate == '' and 'editor' in data_row.keys():\n",
    "            editor_candidate = data_row['editor']\n",
    "        if editor_candidate in editors.keys():\n",
    "            editor_id = editors[editor_candidate]\n",
    "        else:\n",
    "            editor_id = editors['David Zbíral']\n",
    "\n",
    "        import_audit = IOF.make('IAudit')\n",
    "        import_audit['id'] = get_uuid_id()\n",
    "        import_audit['entityId'] = entity_id\n",
    "        import_audit['user'] =  editors['import']\n",
    "        import_audit['date'] = date\n",
    "        import_audit['changes'] = object\n",
    "\n",
    "        editor_audit = IOF.make('IAudit')\n",
    "        editor_audit['id'] = get_uuid_id()\n",
    "        editor_audit['entityId'] = entity_id\n",
    "        editor_audit['user'] =  editor_id\n",
    "        editor_audit['date'] = date\n",
    "        editor_audit['changes'] = object\n",
    "\n",
    "        # audits.append(import_audit)\n",
    "        audits.append(editor_audit)\n",
    "\n",
    "\n",
    "class TerritoryEntityMapper(EntityMapper):\n",
    "    def __init__(self,entity_type, data_row, logger = d.logger):\n",
    "        EntityMapper.__init__(self,entity_type, data_row, logger)\n",
    "\n",
    "        if \"legacyId\" in data_row and \"T\" in data_row[\"legacyId\"]:\n",
    "            self.entity['data']['parent']['order'] = int(data_row[\"legacyId\"].replace(\"T\",\"\"))\n",
    "        else:\n",
    "            # logger.info(f\"In territory entity mapper, legacyId uknown, the order of the entity must be set manually.\")\n",
    "            pass\n",
    "\n",
    "class ConceptEntityMapper(EntityMapper):\n",
    "    def __init__(self,entity_type, data_row, logger = d.logger):\n",
    "        EntityMapper.__init__(self,entity_type, data_row, logger)\n",
    "\n",
    "class ActionEntityMapper(EntityMapper):\n",
    "    def __init__(self,entity_type, data_row, logger = d.logger,):\n",
    "        EntityMapper.__init__(self,entity_type, data_row, logger)\n",
    "\n",
    "    def update_subject_entity_type(self, operation, value, entity_mapper):\n",
    "        # self.logger.info(f\"AP custom field: subject_entity_type\")\n",
    "        entities = [e.strip() for e in value.split(\"|\")]\n",
    "        for e in entities:\n",
    "            if e in self.valid_entity_classes:\n",
    "                self.entity['data']['entities']['s'].append(e)\n",
    "            elif e == \"*\":\n",
    "                self.entity['data']['entities']['s'] = self.valid_entity_classes\n",
    "            else:\n",
    "                logger.error(f\"Non valid entity processed: {e} while AP.update_subject_entity_type().\")\n",
    "\n",
    "    def update_subject_valency(self, operation, value, entity_mapper):\n",
    "        # self.logger.info(f\"AP custom field: subject_valency\")\n",
    "        self.entity['data']['valencies']['s'] = value\n",
    "\n",
    "    def update_actant1_entity_type(self, operation, value, entity_mapper):\n",
    "        # self.logger.info(f\"AP custom field: actant1_entity_type\")\n",
    "        entities = [e.strip() for e in value.split(\"|\")]\n",
    "        for e in entities:\n",
    "            if e in self.valid_entity_classes:\n",
    "                self.entity['data']['entities']['a1'].append(e)\n",
    "            elif e == \"*\":\n",
    "                self.entity['data']['entities']['a1'] = self.valid_entity_classes\n",
    "            else:\n",
    "                logger.error(f\"Non valid entity processed: {e} while AP.update_actant1_entity_type().\")\n",
    "\n",
    "    def update_actant2_entity_type(self, operation, value, entity_mapper):\n",
    "        # self.logger.info(f\"AP custom field: actant2_entity_type\")\n",
    "        entities = [e.strip() for e in value.split(\"|\")]\n",
    "        for e in entities:\n",
    "            if e in self.valid_entity_classes:\n",
    "                self.entity['data']['entities']['a2'].append(e)\n",
    "            elif e == \"*\":\n",
    "                self.entity['data']['entities']['a2'] = self.valid_entity_classes\n",
    "            else:\n",
    "                logger.error(f\"Non valid entity processed: {e} while AP.update_actant2_entity_type().\")\n",
    "\n",
    "    def update_actant1_valency(self, operation, value, entity_mapper):\n",
    "        # self.logger.info(f\"AP custom field: actant1_valency\")\n",
    "        self.entity['data']['valencies']['a1'] = value\n",
    "\n",
    "    def update_actant2_valency(self, operation, value, entity_mapper):\n",
    "        # self.logger.info(f\"AP custom field: actant1_valency\")\n",
    "        self.entity['data']['valencies']['a2'] = value\n",
    "\n",
    "\n",
    "class ResourceEntityMapper(EntityMapper):\n",
    "    def __init__(self,entity_type, data_row, logger = d.logger):\n",
    "        EntityMapper.__init__(self,entity_type, data_row, logger)\n",
    "\n",
    "    def update_url(self, operation, value, entity_mapper):\n",
    "        # self.logger.info(f\"AP custom field: subject_entity_type\")\n",
    "        self.entity['data']['url'] = value\n",
    "\n",
    "\n",
    "class ObjectEntityMapper(EntityMapper):\n",
    "    def __init__(self,entity_type, data_row, logger = d.logger):\n",
    "        EntityMapper.__init__(self,entity_type, data_row, logger)\n",
    "\n",
    "class EventEntityMapper(EntityMapper):\n",
    "    def __init__(self,entity_type, data_row, logger = d.logger):\n",
    "        EntityMapper.__init__(self,entity_type, data_row, logger)\n",
    "\n",
    "class LocationEntityMapper(EntityMapper):\n",
    "    def __init__(self,entity_type, data_row, logger = d.logger):\n",
    "        EntityMapper.__init__(self,entity_type, data_row, logger)\n",
    "\n",
    "class PersonEntityMapper(EntityMapper):\n",
    "    def __init__(self,entity_type, data_row, logger = d.logger):\n",
    "        EntityMapper.__init__(self,entity_type, data_row, logger)\n",
    "\n",
    "\n",
    "class EntityMapperFactory:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def make(self, entity_name, data_row):\n",
    "        if 'ITerritory' == entity_name:\n",
    "            return TerritoryEntityMapper(entity_name, data_row)\n",
    "        elif 'IAction' == entity_name:\n",
    "            return ActionEntityMapper(entity_name, data_row)\n",
    "        elif 'IConcept' == entity_name:\n",
    "            return ConceptEntityMapper(entity_name,data_row)\n",
    "        elif 'IResource' == entity_name:\n",
    "            return ResourceEntityMapper(entity_name, data_row)\n",
    "        elif 'IObject' == entity_name:\n",
    "            return ObjectEntityMapper(entity_name, data_row)\n",
    "        elif 'IEvent' == entity_name:\n",
    "            return EventEntityMapper(entity_name, data_row)\n",
    "        elif 'ILocation' == entity_name:\n",
    "            return LocationEntityMapper(entity_name, data_row)\n",
    "        elif 'IPerson' == entity_name:\n",
    "            return LocationEntityMapper(entity_name, data_row)\n",
    "\n",
    "        else:\n",
    "            logger.warning(f\"Unrecognized entity class in entity mapper. Is this right?\")\n",
    "            return EntityMapper(entity_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# CONTROL CLASS\n",
    "class ParseController():\n",
    "\n",
    "    def __init__(self, entity_list = [], existing_parser = False, reparse_entity_list = [],  keyword_row_id = 3,  logger = d.logger):\n",
    "        self.entity_list = entity_list\n",
    "        self.logger = logger\n",
    "        self.parsers = {}\n",
    "        self.js_objects = []\n",
    "        self.existing_parser = existing_parser\n",
    "        self.reparse_entity_list = reparse_entity_list\n",
    "\n",
    "        if existing_parser and isinstance(existing_parser, ParseController):\n",
    "\n",
    "            self.parsers = existing_parser.parsers\n",
    "            self.logger = existing_parser.logger\n",
    "            self.js_objects = existing_parser.js_objects\n",
    "\n",
    "            for e in self.entity_list:\n",
    "                if 'texts' in e and 'texts' in self.reparse_entity_list:\n",
    "                    self.parsers[e] = TextParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "                elif 'actions' in e and 'actions' in self.reparse_entity_list:\n",
    "                    self.parsers[e] = ActionParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "                elif 'concepts' in e and 'concepts' in self.reparse_entity_list:\n",
    "                    self.parsers[e] = ConceptParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "                elif 'resources' in e and 'resources' in self.reparse_entity_list:\n",
    "                    self.parsers[e] = ResourceParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "                elif 'manuscripts' in e and 'manuscripts' in self.reparse_entity_list:\n",
    "                    self.parsers[e] = ManuscriptParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "                elif 'R0006_persons' in e and 'R0006_persons' in self.reparse_entity_list:\n",
    "                    self.parsers[e] = PersonParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "                elif 'R0007_locations' in e and 'R0007_locations' in self.reparse_entity_list:\n",
    "                    self.parsers[e] = LocationParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "                elif 'R0008_events' in e and 'R0008_events' in self.reparse_entity_list:\n",
    "                    self.parsers[e] = EventParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "                elif 'R0075_persons' in e and 'R0075_persons' in self.reparse_entity_list:\n",
    "                    self.parsers[e] = PersonParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "                elif 'R0035_locations' in e and 'R0035_locations' in self.reparse_entity_list:\n",
    "                    self.parsers[e] = LocationParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "                elif 'R0083_events' in e and 'R0083_events' in self.reparse_entity_list:\n",
    "                    self.parsers[e] = EventParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "                else:\n",
    "                    self.logger.warning(f\"Coming to basic Parser entity - strange '{e}' {type(e)}.\")\n",
    "                    self.parsers[e] = Parser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "\n",
    "        else:\n",
    "            for e in self.entity_list:\n",
    "                if 'texts' in e:\n",
    "                    self.parsers[e] = TextParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "                elif 'actions' in e:\n",
    "                    self.parsers[e] = ActionParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "                elif 'concepts' in e:\n",
    "                    self.parsers[e] = ConceptParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "                elif 'resources' in e:\n",
    "                    self.parsers[e] = ResourceParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "                elif 'manuscripts' in e:\n",
    "                    self.parsers[e] = ManuscriptParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "                elif 'R0006_persons' in e:\n",
    "                    self.parsers[e] = PersonParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "                elif 'R0007_locations' in e:\n",
    "                    self.parsers[e] = LocationParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "                elif 'R0008_events' in e:\n",
    "                    self.parsers[e] = EventParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "                elif 'R0075_persons' in e:\n",
    "                    self.parsers[e] = PersonParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "                elif 'R0035_locations' in e:\n",
    "                    self.parsers[e] = LocationParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "                elif 'R0083_events' in e:\n",
    "                    self.parsers[e] = EventParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "                else:\n",
    "                    self.logger.warning(f\"Coming to basic Parser entity - strange '{e}' {type(e)}.\")\n",
    "                    self.parsers[e] = Parser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "\n",
    "    def load_json_objects(self):\n",
    "        for name, p in self.parsers.items():\n",
    "            self.js_objects = self.js_objects + p.js_objects\n",
    "\n",
    "    def parse(self, stop = False):\n",
    "        for name, p in self.parsers.items():\n",
    "            if len(self.reparse_entity_list) > 0:\n",
    "                if name in self.reparse_entity_list:\n",
    "                    self.logger.info(\"PARTIAL PARSE Active, reparsing \"+name)\n",
    "                    p.parse_rows(stop)\n",
    "                else:\n",
    "                    self.logger.info(\"PARTIAL PARSE Active, skipping \"+name)\n",
    "            else:\n",
    "                p.parse_rows(stop)\n",
    "\n",
    "\n",
    "# WORKER CLASS\n",
    "class Parser():\n",
    "    EMP = EntityMapperFactory()\n",
    "\n",
    "    def __init__(self, name, header_df: pd.DataFrame, table_df: pd.DataFrame, keyword_row_id: int, logger: logger):\n",
    "        self.name = name\n",
    "        self.logname = name.upper()\n",
    "        self.input_header_df = header_df\n",
    "        self.input_table_df = table_df\n",
    "        self.prepared_table = pd.DataFrame()\n",
    "        self.keyword_row_id =  keyword_row_id\n",
    "        self.columns = self.input_header_df.columns.tolist()\n",
    "\n",
    "        self.proptype_2nd = {} # for registering the columns, which contains proptype_2nd information\n",
    "\n",
    "        self.parsing_instruction = {}\n",
    "        self.oper_columns = {'discard':[],'inside':[],'special':[],'unknown':[],\"proptype\":[],'propvalue':[],'proptype_2nd':[],'propvalue_2nd':[],\"dependent\":[], \"reference\":[], \"reference_object\":[], \"reference_part\":[],\"hooked-inside\":[],\"hooked-propvalue\":[],\"hooked-relation\":[]}\n",
    "        self.logger = logger\n",
    "\n",
    "        # parsed json data holder\n",
    "        self.js_objects = []\n",
    "\n",
    "        # RUN\n",
    "        self.process_header_instructions()\n",
    "        self.prepare_input_table()\n",
    "\n",
    "    # \"parsing\" instructions\n",
    "    def process_header_instructions(self) -> (pd.DataFrame, pd.DataFrame):\n",
    "        keyword_row = self.input_header_df.iloc[self.keyword_row_id]\n",
    "        prop_type_row = self.input_header_df.iloc[self.keyword_row_id - 1]\n",
    "        source_node_row = self.input_header_df.iloc[self.keyword_row_id - 2]\n",
    "\n",
    "        log_uncertain_instructions = []\n",
    "\n",
    "        for c in self.columns:\n",
    "            instruction_candidate = str(keyword_row.at[c]).strip()\n",
    "            prop_type_candidate = str(prop_type_row.at[c]).strip()\n",
    "            source_node_candidate = str(source_node_row.at[c]).strip()\n",
    "\n",
    "            if c == '':\n",
    "                self.logger.error(f\"{self.logname} There is empty column in the dataset.\")\n",
    "                raise Exception(f\"{self.logname} There is empty column in the dataset.\")\n",
    "\n",
    "            if \"?\" in instruction_candidate or \"?\" in prop_type_candidate or \"?\" in source_node_candidate:\n",
    "                log_uncertain_instructions.append(f\"{c.upper()}:{instruction_candidate},{prop_type_candidate},{source_node_candidate}\")\n",
    "                instruction  = {'operation':'discard', 'target': None}\n",
    "                self.oper_columns['discard'].append(c)\n",
    "\n",
    "\n",
    "            # known instructions\n",
    "            if 'discard' in instruction_candidate:\n",
    "                instruction  = {'operation':'discard', 'target': None}\n",
    "                self.oper_columns['discard'].append(c)\n",
    "\n",
    "            elif instruction_candidate == \"propvalue\":\n",
    "                prop_type = prop_type_candidate\n",
    "                source_node = source_node_candidate\n",
    "\n",
    "                if source_node != \"\" and prop_type == \"\":\n",
    "                    prop_type = source_node\n",
    "\n",
    "                # test whether is its propvalue proper or dependent (=proptype is dynamic, value from another column)\n",
    "                if prop_type.strip() == \"\":\n",
    "                    self.oper_columns['dependent'].append(c)\n",
    "                    logger.info(f\"Throwing out propvalue instruction in column {c}.\")\n",
    "                    continue # ignoring \"dependent propvalue\"\n",
    "\n",
    "                if \"?\" in prop_type or \"?\"  in source_node:\n",
    "                    instruction = {'operation':'unknown', 'target': None}\n",
    "                    self.oper_columns['unknown'].append(c)\n",
    "                else:\n",
    "                    instruction  = {'operation':'propvalue', 'type': prop_type, 'source':source_node} # source can be ignored, because the iProp object is sitting inside of it\n",
    "                    self.oper_columns['propvalue'].append(c)\n",
    "\n",
    "            elif 'propvalue_2nd' in instruction_candidate:\n",
    "                prop_type = prop_type_candidate\n",
    "                source_node = source_node_candidate\n",
    "                if \"?\" in prop_type or \"?\"  in source_node:\n",
    "                    instruction = {'operation':'unknown', 'target': None}\n",
    "                    self.oper_columns['unknown'].append(c)\n",
    "                else:\n",
    "                    instruction  = {'operation':'propvalue_2nd', 'type': prop_type, 'source':source_node} # source can NOT be ignored, it signals which existing iProp object will hold this iProp object\n",
    "                    self.oper_columns['propvalue_2nd'].append(c)\n",
    "\n",
    "            elif 'proptype_2nd' in instruction_candidate:\n",
    "                prop_type = prop_type_candidate\n",
    "                source_node = source_node_candidate\n",
    "                if \"?\" in prop_type or \"?\"  in source_node:\n",
    "                    instruction = {'operation':'unknown', 'target': None}\n",
    "                    self.oper_columns['unknown'].append(c)\n",
    "                else:\n",
    "                    instruction  = {'operation':'proptype_2nd', 'type': prop_type, 'source':source_node}\n",
    "                    self.oper_columns['proptype_2nd'].append(c)\n",
    "\n",
    "            elif 'special' == instruction_candidate:\n",
    "                # looks for custom functions registered by column name\n",
    "                prop_type = prop_type_candidate\n",
    "                source_node = source_node_candidate\n",
    "                instruction  = {'operation':'special', 'type': prop_type, 'source':source_node}\n",
    "                self.oper_columns['special'].append(c)\n",
    "\n",
    "            elif 'proptype' in instruction_candidate:\n",
    "                prop_type = prop_type_candidate\n",
    "                source_node = source_node_candidate\n",
    "                instruction  = {'operation':'proptype', 'type': prop_type, 'source':source_node}\n",
    "                #logger.info(f\"here ...{instruction_candidate} {c}\")\n",
    "                self.oper_columns['proptype'].append(c)\n",
    "\n",
    "            elif 'dependent' in instruction_candidate:\n",
    "                # ignore\n",
    "                # the value is solved by another instruction\n",
    "                instruction  = {'operation':'dependent', 'type': prop_type, 'source':source_node}\n",
    "\n",
    "            elif instruction_candidate == \"inside\":\n",
    "                if \"?\" in c:\n",
    "                    instruction = {'operation':'unknown', 'target': None}\n",
    "                    self.oper_columns['unknown'].append(c)\n",
    "                else:\n",
    "                    instruction  = {'operation':'inside', 'target': None}\n",
    "                    if len(prop_type_candidate) > 0:\n",
    "                        instruction  = {'operation':'inside', 'target': prop_type_candidate}\n",
    "\n",
    "                    self.oper_columns['inside'].append(c)\n",
    "\n",
    "            elif instruction_candidate == \"hooked-inside\":\n",
    "                if \"?\" in c:\n",
    "                    instruction = {'operation':'unknown', 'target': None}\n",
    "                    self.oper_columns['unknown'].append(c)\n",
    "                else:\n",
    "                    instruction  = {'operation':'hooked-inside', 'target': None}\n",
    "                    if len(prop_type_candidate) > 0:\n",
    "                        instruction  = {'operation':'hooked-inside', 'target': prop_type_candidate}\n",
    "\n",
    "                    self.oper_columns['hooked-inside'].append(c)\n",
    "\n",
    "            elif instruction_candidate == \"hooked-propvalue\":\n",
    "                if \"?\" in c:\n",
    "                    instruction = {'operation':'unknown', 'target': None}\n",
    "                    self.oper_columns['unknown'].append(c)\n",
    "                else:\n",
    "                    instruction  = {'operation':'hooked-propvalue', 'target': None}\n",
    "                    if len(prop_type_candidate) > 0:\n",
    "                        instruction  = {'operation':'hooked-propvalue', 'target': prop_type_candidate}\n",
    "\n",
    "                    self.oper_columns['hooked-propvalue'].append(c)\n",
    "\n",
    "            elif instruction_candidate == \"hooked-relation\":\n",
    "                if \"?\" in c:\n",
    "                    instruction = {'operation':'unknown', 'target': None}\n",
    "                    self.oper_columns['unknown'].append(c)\n",
    "                else:\n",
    "                    instruction  = {'operation':'hooked-relation', 'target': None}\n",
    "                    if len(prop_type_candidate) > 0:\n",
    "                        instruction  = {'operation':'hooked-relation', 'target': prop_type_candidate}\n",
    "\n",
    "                    self.oper_columns['hooked-relation'].append(c)\n",
    "\n",
    "            elif \"reference\" in instruction_candidate:\n",
    "                #logger.info(f\"Found instruction 'reference'. {prop_type_candidate}\")\n",
    "                self.oper_columns[\"reference\"].append(c)\n",
    "                instruction  = {'operation':'reference', 'ref_legacy_id':prop_type_candidate}\n",
    "\n",
    "            elif \"reference_object\" in instruction_candidate:\n",
    "                #logger.info(f\"Found instruction 'reference'. {prop_type_candidate}\")\n",
    "                self.oper_columns[\"reference_object\"].append(c)\n",
    "                instruction  = {'operation':'reference_object', 'ref_legacy_id':prop_type_candidate}\n",
    "\n",
    "            elif \"reference_part\" in instruction_candidate:\n",
    "                #logger.info(f\"Found instruction 'reference'. {prop_type_candidate}\")\n",
    "                self.oper_columns[\"reference_part\"].append(c)\n",
    "                instruction  = {'operation':'reference_part', 'ref_legacy_id':prop_type_candidate}\n",
    "\n",
    "            elif instruction_candidate == \"relation\":\n",
    "                relation_type = prop_type_candidate\n",
    "                instruction  = {'operation':'relation', 'type': relation_type}\n",
    "\n",
    "            else:\n",
    "                instruction = {'operation':'unknown', 'target': None}\n",
    "                self.oper_columns['unknown'].append(c)\n",
    "            self.parsing_instruction[c] = instruction\n",
    "\n",
    "        self.logger.info(f\"{self.logname} Uncertain parsing instructions in {len(log_uncertain_instructions)} columns: \" + \" \".join(log_uncertain_instructions) + \".\")\n",
    "        return self.parsing_instruction\n",
    "\n",
    "    def prepare_input_table(self):\n",
    "        ip = self.input_table_df.copy()\n",
    "\n",
    "        # discard  columns with discard and unknown operations\n",
    "        for c in self.oper_columns['discard']+self.oper_columns['unknown']:\n",
    "            ip.drop(columns=c, inplace=True)\n",
    "        #ip.drop(columns=self.oper_columns['discard']+self.oper_columns['unknown'], inplace=True)\n",
    "\n",
    "        self.logger.info(f\"{self.logname} {len(self.oper_columns['discard']+self.oper_columns['unknown'])} columns have been dropped (discard:{len(self.oper_columns['discard'])}, unknown:{len(self.oper_columns['unknown'])}: {self.oper_columns['unknown']}). Table now has {len(ip.columns)} columns, inside:{len(self.oper_columns['inside'])},propvalue:{len(self.oper_columns['propvalue'])}, special:{len(self.oper_columns['special'])}, proptype: {len(self.oper_columns['proptype'])}, proptype_2nd: {len(self.oper_columns['proptype_2nd'])}, dependent:{len(self.oper_columns['dependent'])}, reference_part:{len(self.oper_columns['reference_part'])}. Originally {self.input_table_df.shape[1]} columns.\")\n",
    "\n",
    "        self.prepared_table = ip\n",
    "\n",
    "    def prepare_property(self):\n",
    "        pass\n",
    "\n",
    "    def make_row_object(self, data_row):\n",
    "        class_name = table_to_entity[self.name]\n",
    "        return self.EMP.make(class_name, data_row)\n",
    "\n",
    "    def itemize_valuestring_for_multiples(self, value_with_multiples, origin=\"\") -> []:\n",
    "        values = []\n",
    "        if isinstance(value_with_multiples,str):\n",
    "            parsed_value = value_with_multiples.split('#')\n",
    "            values =  [item.strip() for item in parsed_value]\n",
    "        else:\n",
    "            raise Exception(f\"Expected value to be string. Got {type(value_with_multiples)}. {origin}\")\n",
    "        return values\n",
    "\n",
    "    def parse_rows(self, stop = False):\n",
    "        self.logger.info(f\"Starting to parse {self.name}.\")\n",
    "\n",
    "        if  self.prepared_table['label'].isnull().any():\n",
    "            self.prepared_table  = self.prepared_table[self.prepared_table['label'].notna()]\n",
    "            self.logger.info(f\"Empty labels found in {self.name} table. (new entities added through parsing process). Adjusting - entities with empty labels not taken into account.\")\n",
    "\n",
    "        counter = 0\n",
    "        for key, row in self.prepared_table.iterrows():\n",
    "            counter += 1\n",
    "            if stop and counter > stop:\n",
    "                break\n",
    "            #self.logger.info(f\"{self.name} Processing row {key}\")\n",
    "            #self.logger.info(f\"{row.to_dict()}\")\n",
    "            entity_mapper = self.make_row_object(row.to_dict())\n",
    "\n",
    "            if row['legacyId'] == \"\":\n",
    "                logger.info(f\"Skipping {row['label']}, does not have set legacyId. Remnant of already run parse?\")\n",
    "                continue\n",
    "\n",
    "            for name, value in row.items():\n",
    "                if name in self.parsing_instruction:\n",
    "                    operation = self.parsing_instruction[name]\n",
    "                else:\n",
    "                    continue # silently ignore unknown columns\n",
    "                # logger.info(f\"{self.name} Processing columns {name}, with value {value}. Op:{operation}\")\n",
    "\n",
    "                # force string\n",
    "                value = str(value)\n",
    "\n",
    "                # validation of value for question marks\n",
    "                if \"??\" in value:\n",
    "                    logger.info(f\"About ??? : There is {value} in column {name} in row {str(key)}.\")\n",
    "                    continue\n",
    "\n",
    "                # thrashing \"NA\"\n",
    "                #if value == \"NA\":\n",
    "                #    continue\n",
    "\n",
    "                if operation['operation'] == 'inside' and value != '' and '?' not in name:\n",
    "                    # logger.info(f\"{self.name} Processing columns {name}, with value {value}. Op:{operation}\")\n",
    "                    if operation['target']:\n",
    "                        name = operation['target']\n",
    "                    entity_mapper.update_inside_field(name,value,operation['operation'] +\">\"+ str(key)+\":\"+str(name)+\":\"+str(value))\n",
    "\n",
    "                if operation['operation'] == 'propvalue' and value != '':\n",
    "\n",
    "                    #logger.info(f\"{self.name} Processing 'propvalue' for column {name}, with value {value} and type {operation['type']} Op:{operation}\")\n",
    "                    prop_type = operation['type']\n",
    "\n",
    "                    if prop_type in row.keys():  # prop type is defined in other column (it is NOT fixed for all propvalues)\n",
    "                        prop_type = row[prop_type]\n",
    "                        # logger.info(f\"Shifting to remote proptype {prop_type} in the context of {name}, {value},{operation['type']}.\")\n",
    "\n",
    "                    if prop_type == '' or 'C' not in prop_type:\n",
    "                        raise Exception(f\"Propvalue does not have prop type '{prop_type}' defined. C entity-string expected, got {key}, {name}, {value}. [{operation}]\")\n",
    "\n",
    "                    for item in self.itemize_valuestring_for_multiples(value):\n",
    "                        #logger.info(f\"Propvalue value {value} item {item}.\")\n",
    "                        entity_mapper.hook_prop_object(prop_type = prop_type, input_value = item, origin = operation['operation'] +\">\"+ str(key)+f\":{row['legacyId']}\"+\":\"+str(name)+\":\"+str(value), field_name = name)\n",
    "\n",
    "\n",
    "                if operation['operation'] == 'proptype_2nd' and value != '':\n",
    "                    # logger.info(f\"{self.name} Processing columns {name}, with value {value}. Op:{operation}\")\n",
    "\n",
    "                    #register proptype_2nd for the source\n",
    "                    self.proptype_2nd[operation['source']] = name\n",
    "\n",
    "\n",
    "                    # prop_type = operation['type']\n",
    "                    # if prop_type == '' or 'C' not in prop_type:\n",
    "                    #     raise Exception(f\"Propvalue does not have prop type defined. C entity-string expected, got {key}, {name}, {value}\")\n",
    "                    # for item in self.itemize_valuestring_for_multiples(value):\n",
    "                    #     entity_mapper.hook_prop_object(prop_type = prop_type, input_value = item, origin = operation['operation'] +\">\"+ str(key)+\":\"+str(name)+\":\"+str(value), field_name = name)\n",
    "\n",
    "                if operation['operation'] == 'propvalue_2nd' and value != '':\n",
    "                    #logger.info(f\"{self.name} propvalue_2nd : Processing columns {name}, with value {value}. Op:{operation}\")\n",
    "                    prop_type = operation['type']\n",
    "                    prop_source_name = operation['source']  # header_name,  we need concept_id\n",
    "                    prop_source_id = self.parsing_instruction[prop_source_name]['type']\n",
    "\n",
    "                    #logger.info(f\"OP provalue_2nd with prop_source_name: '{prop_source_name}' : prop_source_id '{prop_source_id}' : value '{value}'\")\n",
    "\n",
    "                    #logger.info(f\"There are registered proptype_2nd: {self.proptype_2nd}\")\n",
    "                    # logger.info(f\"There are parsing instructions : {self.parsing_instruction}\")\n",
    "\n",
    "                    # assert 'C' in prop_source_id, f\"Trying to get to the concept of 1st level property to address 2nd level property. Context {self.name} propvalue_2nd : Processing columns {name}, with value {value}. Op:{operation} in {row}.\"\n",
    "\n",
    "                    # hack for Castellario events? other datasets do not need it ? why????\n",
    "                    #if prop_type == '' and prop_source_id == '':\n",
    "                    #    prop_type = row[prop_source_name]\n",
    "\n",
    "                    # there can be two situations\n",
    "                    # B - proptype is defined in proptype_2nd column\n",
    "                    if prop_source_name in self.proptype_2nd.keys():\n",
    "                        # logger.info(f\"We have registered source type column {self.proptype_2nd[prop_source_name]} for this propvalue_2nd.\")\n",
    "                        prop_type = row[self.proptype_2nd[prop_source_name]]\n",
    "                    else:\n",
    "                        # A - proptype is fixed for the whole column, defined in proptype_2nd\n",
    "                        if prop_type == '' or 'C' not in prop_type:\n",
    "                            raise Exception(f\"Propvalue_2nd does not have prop type defined. C entity-string expected, got prop_type '{prop_type}', key:{key}, name:{name}, value:{value}\")\n",
    "\n",
    "                    origin = operation['operation'] +\">\"+ str(key)+f\":{row['legacyId']}\"+\":\"+str(name)+\":\"+str(value)\n",
    "\n",
    "                    for item in self.itemize_valuestring_for_multiples(value, origin=origin):\n",
    "                        entity_mapper.hook_prop_object(prop_type = prop_type, input_value = item, prop_source = prop_source_id, origin = origin, field_name = name, prop_source_field = prop_source_name)\n",
    "\n",
    "                if operation['operation'] == 'special' and value != \"\":\n",
    "                    # logger.info(f\"SPECIAL {operation} {value}\")\n",
    "                    func = getattr(self, 'special_'+name)\n",
    "                    func(operation, value, entity_mapper)\n",
    "\n",
    "                if operation['operation'] == 'reference' and value != \"\":\n",
    "                    pass\n",
    "                if operation['operation'] == 'reference_object' and value != \"\":\n",
    "                    pass\n",
    "\n",
    "                if operation['operation'] == 'reference_part' and value != \"\":\n",
    "                    ref_object_legacy_id = operation['ref_legacy_id']\n",
    "\n",
    "                    if ref_object_legacy_id in row.keys():\n",
    "                        ref_object_legacy_id = row[ref_object_legacy_id]\n",
    "\n",
    "                    logger.info(f\"Doing {operation}, {ref_object_legacy_id}, '{value}'. Row {row}.\")\n",
    "                    entity_mapper.hook_ref_object(ref_legacyID = ref_object_legacy_id, input_value = value, origin = operation['operation'] +\">\"+ \":\"+ name + \" \" +str(value))\n",
    "\n",
    "                if operation['operation'] == 'relation' and value != \"\" and value != \"NA\":\n",
    "                    relation_type = operation['type']\n",
    "                    related_object = value\n",
    "\n",
    "                    id1 = entity_mapper.entity['id']\n",
    "\n",
    "                    #for order,item in enumerate(reversed(self.itemize_valuestring_for_multiples(related_object)),1):\n",
    "\n",
    "                    if relation_type == \"Synonym\":\n",
    "                        synonym_ids = [id1]\n",
    "                        for item in self.itemize_valuestring_for_multiples(related_object):\n",
    "                            if item[0] not in entity_mapper.valid_entity_classes:\n",
    "                                logger.error(f\"The string {item} does not refer to valid entity class - trying to instantiate relation {relation_type} in row {key}. \")\n",
    "                                continue\n",
    "                            id_other = entity_mapper.get_entity_id(item, origin = str(key)+ \" Getting other relation entity in bilding synonm relation. \"+relation_type+ \" \"+item )\n",
    "                            synonym_ids.append(id_other)\n",
    "\n",
    "                        if len(synonym_ids) > 1:\n",
    "                            entity_mapper.make_relation_synonimic_record(relation_type, synonym_ids, origin=operation['operation'] +\">\"+ \":\"+ relation_type + f\" {entity_mapper.entity['legacyId']}\" +str(synonym_ids))\n",
    "                        else:\n",
    "                            logger.error(f\"Heh? Empty synonym_ids in \"+operation['operation'] +\">\"+ \":\"+ relation_type + f\" {entity_mapper.entity['legacyId']}: \" +str(synonym_ids))\n",
    "\n",
    "                    else:\n",
    "                        for order,item in enumerate(self.itemize_valuestring_for_multiples(related_object),1):\n",
    "                            if item[0] not in entity_mapper.valid_entity_classes:\n",
    "                                logger.error(f\"The string {item} does not refer to valid entity class - trying to instantiate relation {relation_type} in row {key}. \")\n",
    "                                continue\n",
    "                            id2 = entity_mapper.get_entity_id(item, origin = str(key)+ \" Getting second relation entity. \"+relation_type+ \" \"+item )\n",
    "                            entity_mapper.make_relation_record(relation_type, [id1, id2], origin=operation['operation'] +\">\"+ \":\"+ relation_type + f\" {entity_mapper.entity['legacyId']}>{item} \" +str(id1)+\" \"+str(id2), order=order*10)\n",
    "\n",
    "            self.js_objects.append(entity_mapper.entity)\n",
    "\n",
    "            # make audit records\n",
    "            entity_mapper.create_audit_record()\n",
    "\n",
    "            # break  DEV, checking parsing after first iteration\n",
    "\n",
    "\n",
    "    def special_editor(self, operation, value, entity_mapper, field_name=\"special_editor\"):\n",
    "        pass\n",
    "\n",
    "\n",
    "class TextParser(Parser):\n",
    "\n",
    "    def __init__(self, name, header_df: pd.DataFrame, table_df: pd.DataFrame, keyword_row_id: int, logger: logger):\n",
    "        Parser.__init__(self, name, header_df, table_df, keyword_row_id, logger)\n",
    "\n",
    "    # special methods for fields, which needs fully individual processing\n",
    "    def special_edition_1(self, operation, value, entity_mapper, field_name=\"edition_1\", ):\n",
    "        # Parse this col. as \"propvalue\" - but you need to generate the target entities since they do not exist. How to do it: for any value, create an R entity with \"label\" = textual value in this col., \"label language\" = English, \"status\" = \"approved\", and \"URL\" = the hyperlink in the formula sitting on the textual value in this col. As usual, ignore NS and NA values (exact match) - do not import anything if the value is NA.\n",
    "        # logger.info(f\"Special edition1 running ...{operation} {value}\")\n",
    "\n",
    "        origin = operation['operation'] +\" \"+ operation['type']+ \">\"+ \":\"+field_name + str(value)\n",
    "        prop_type = operation['type']\n",
    "\n",
    "        # make rentity\n",
    "        # logger.info(f\"Rentity making {value}\")\n",
    "        if \"|\" in value:\n",
    "            data = value.split(\"|\")\n",
    "            label = data[0]\n",
    "            url = data[1]\n",
    "        else:\n",
    "            url = \"\"\n",
    "            label = value\n",
    "            # logger.warning(f\"Expected char | signaling url after label. Got just {value}.\"+origin)\n",
    "\n",
    "        # generate resource entity if it does not exist\n",
    "        # check by label and url\n",
    "        tdf = tables['resources'].fillna(\"\")\n",
    "        check_rentity = tdf[(tdf['label'] == label) & (tdf['dissinet_repository_url'] == url)]\n",
    "        #check_rentity = tdf[(tdf['label'] == label)  & (~tdf['dissinet_repository_url'].isna() | ~(tdf['dissinet_repository_url']==\"\"))]\n",
    "        if check_rentity.empty:\n",
    "            rentity = entity_mapper.make_rentity(label, url, origin=origin)\n",
    "            rentity['language'] = entity_mapper.enum_mapper['language']['English']\n",
    "        else:\n",
    "            rentity = {'id': check_rentity.iloc[0]['id']}\n",
    "\n",
    "        entity_mapper.create_audit_record(entity_id=rentity['id'], object=rentity)\n",
    "        entity_mapper.hook_prop_object(prop_type = prop_type, input_value = rentity['id'], origin = origin, field_name=field_name)\n",
    "\n",
    "    def special_edition_2(self, operation, value, entity_mapper):\n",
    "        self.special_edition_1( operation, value, entity_mapper, field_name=\"edition_2\")\n",
    "\n",
    "    def special_edition_3(self, operation, value, entity_mapper):\n",
    "        self.special_edition_1( operation, value, entity_mapper, field_name=\"edition_3\")\n",
    "\n",
    "    def special_creation_event_id(self, operation, value, entity_mapper : EntityMapper, field_name=\"creation_event_id\"):\n",
    "        # old instructions\n",
    "        # Create entities in this col. as new E entities, with (1) the value here as legacy_id, (2) assign (as usual) a new “hash” ID from the db, (3) label of this E: see next col., (4) logical type “definite” (default), (5) label language “English”, (6) status “approved”, and (7) attach to any of those Es the metaprop \"(has) - C0565 “class” - C2642 “creation” (to instantiate the event to its event type = event class).\n",
    "\n",
    "        # new instructionds\n",
    "        # DONE\n",
    "        # Create entities in this col. as new E entities, with (1) the value here as legacy_id, (2) assign (as usual) a new \"hash\" ID from the db, (3) label of this E: see next col., (4) logical type \"definite\" (default), (5) label language \"English\", (6) status \"approved\", and (7) attach to this E the Relation of the type Classification leading to C2642 \"creation\".\n",
    "\n",
    "        data = entity_mapper.data_row\n",
    "        origin = operation['operation']+ \" \" + data['legacyId'] + \">\"+ \":\"+field_name +\" \"+ str(value)\n",
    "\n",
    "        event_entity = entity_mapper.make_eentity(data['creation_event_label'], legacyId = value, origin = origin)\n",
    "        event_entity['language'] = entity_mapper.enum_mapper['language']['English']\n",
    "\n",
    "        # prop_type_id = entity_mapper.get_entity_id(\"C0565\")\n",
    "        # prop_value_id = entity_mapper.get_entity_id(\"C2642\")\n",
    "        # make IProp object\n",
    "        # prop_object = entity_mapper.make_prop_object(prop_type_id, prop_value_id)\n",
    "        # hook prop object to the event entity\n",
    "        # event_entity['props'].append(prop_object)\n",
    "        # hook the vent event to the territory\n",
    "        # entity_mapper.hook_prop_object(prop_type = \"C2642\", input_value = event_entity['id'], origin = origin, field_name=field_name)\n",
    "\n",
    "        # make event part of territories props\n",
    "        entity_mapper.hook_prop_object(prop_type = \"C2642\", input_value = event_entity['id'], origin = origin, field_name=field_name)\n",
    "        # (7) attach to this E the Relation of the type Classification leading to C2642 \"creation\"\n",
    "        entity_mapper.make_relation_record('Classification',[event_entity['id'],entity_mapper.get_entity_id(\"C2642\")])\n",
    "\n",
    "        # process time_relations\n",
    "        t_relations = [(\n",
    "            data[\"timerelation1_type_conceptified_id\"], data[\"timerelation1_target_id\"]\n",
    "        ), (\n",
    "            data[\"timerelation2_type_conceptified_id\"], data[\"timerelation2_target_id\"]\n",
    "        ),(\n",
    "            data[\"timerelation3_type_conceptified_id\"], data[\"timerelation3_target_id\"]\n",
    "        ), (\n",
    "            data[\"timerelation4_type_conceptified_id\"], data[\"timerelation4_target_id\"]\n",
    "        )]\n",
    "\n",
    "        for o in t_relations:\n",
    "            if len(o[0]) > 0 and len(o[1]) > 0:\n",
    "                prop_type_id = entity_mapper.get_entity_id(o[0], origin = origin)\n",
    "                prop_value_id = entity_mapper.get_entity_id(\"~V~\"+o[1], origin = origin)\n",
    "                prop_object = entity_mapper.make_prop_object(prop_type_id, prop_value_id)\n",
    "                event_entity['props'].append(prop_object)\n",
    "\n",
    "        # create audit record\n",
    "        entity_mapper.create_audit_record(entity_id=event_entity['id'], object=event_entity)\n",
    "\n",
    "\n",
    "    # empty, operation is solved by f above\n",
    "    def special_creation_event_label(self, operation, value, entity_mapper):\n",
    "        pass\n",
    "\n",
    "\n",
    "class ActionParser(Parser):\n",
    "\n",
    "    def __init__(self, name, header_df: pd.DataFrame, table_df: pd.DataFrame, keyword_row_id: int, logger: logger):\n",
    "        Parser.__init__(self, name, header_df, table_df, keyword_row_id, logger)\n",
    "\n",
    "\n",
    "class ConceptParser(Parser):\n",
    "\n",
    "    def __init__(self, name, header_df: pd.DataFrame, table_df: pd.DataFrame, keyword_row_id: int, logger: logger):\n",
    "        Parser.__init__(self, name, header_df, table_df, keyword_row_id, logger)\n",
    "\n",
    "    def special_wordnet_synset_id(self, operation, value, entity_mapper,field_name=\"special_wordnet_synset_id\",):\n",
    "        # wordnet_resource_id = R0067\n",
    "        entity_mapper.hook_ref_object(ref_legacyID = \"R0067\", input_value = value, origin = operation['operation'] +\">\"+ \":\"+field_name + \" \" +str(value))\n",
    "\n",
    "\n",
    "class ManuscriptParser(Parser):\n",
    "\n",
    "    def __init__(self, name, header_df: pd.DataFrame, table_df: pd.DataFrame, keyword_row_id: int, logger: logger):\n",
    "        Parser.__init__(self, name, header_df, table_df, keyword_row_id, logger)\n",
    "\n",
    "    def special_creation_event_id(self, operation, value, entity_mapper : EntityMapper, field_name=\"creation_event_id\"):\n",
    "        # Create entities in this col. as new E entities, with (1) the value here as legacy_id, (2) assign (as usual) a new “hash” ID from the db, (3) label of this E: see next col., (4) logical type “definite” (default), (5) label language “English”, (6) status “approved”, and (7) attach to any of those Es the metaprop \"(has) - C0565 “class” - C2642 “creation” (to instantiate the event to its event type = event class).\n",
    "\n",
    "        # new instructions\n",
    "        # TODO\n",
    "        # Create entities in this col. as new E entities, with (1) the value here as legacy_id, (2) assign (as usual) a new \"hash\" ID from the db, (3) label of this E: take from next col., (4) logical type \"definite\" (default), (5) label language \"English\", (6) status \"approved\", and (7) attach to this E the Relation of the type Classification leading to C2642 \"creation\".\n",
    "\n",
    "        origin = operation['operation'] +\">\"+ \":\"+field_name +\" \"+ str(value)\n",
    "\n",
    "        data = entity_mapper.data_row\n",
    "        event_entity = entity_mapper.make_eentity(data['creation_event_label'], legacyId = value, origin = origin)\n",
    "        event_entity['language'] = entity_mapper.enum_mapper['language']['English']\n",
    "\n",
    "        # OLD way, we have relations now\n",
    "        #prop_type_id = entity_mapper.get_entity_id(\"C0565\")\n",
    "        #prop_value_id = entity_mapper.get_entity_id(\"C2642\")\n",
    "        # make IProp object\n",
    "        #prop_object = entity_mapper.make_prop_object(prop_type_id, prop_value_id)\n",
    "        # hook prop object to the event entity\n",
    "        #event_entity['props'].append(prop_object)\n",
    "\n",
    "        # hook the vent event to the territory\n",
    "        entity_mapper.hook_prop_object(prop_type = \"C2642\", input_value = event_entity['id'], origin = operation['operation'] +\">\"+ \":\"+field_name + str(value))\n",
    "\n",
    "        # (7) attach to this E the Relation of the type Classification leading to C2642 \"creation\"\n",
    "        entity_mapper.make_relation_record('Classification',[event_entity['id'],entity_mapper.get_entity_id(\"C2642\")])\n",
    "\n",
    "        # process time_relations\n",
    "        t_relations = [(\n",
    "            data[\"timerelation1_type_conceptified_id\"], data[\"timerelation1_target_id\"]\n",
    "        ), (\n",
    "            data[\"timerelation2_type_conceptified_id\"], data[\"timerelation2_target_id\"]\n",
    "        ),(\n",
    "            data[\"timerelation3_type_conceptified_id\"], data[\"timerelation3_target_id\"]\n",
    "        ), (\n",
    "            data[\"timerelation4_type_conceptified_id\"], data[\"timerelation4_target_id\"]\n",
    "        )]\n",
    "\n",
    "        origin = origin + \"LegacyId: \"+ data['legacyId']\n",
    "        for o in t_relations:\n",
    "            if len(o[0]) > 0 and len(o[1]) > 0:\n",
    "                prop_type_id = entity_mapper.get_entity_id(o[0], origin = origin)\n",
    "                prop_value_id = entity_mapper.get_entity_id(\"~V~\"+o[1], origin = origin)\n",
    "                prop_object = entity_mapper.make_prop_object(prop_type_id, prop_value_id)\n",
    "                event_entity['props'].append(prop_object)\n",
    "\n",
    "        # create audit record\n",
    "        entity_mapper.create_audit_record(entity_id=event_entity['id'], object=event_entity)\n",
    "\n",
    "\n",
    "    # empty, operation is solved by f above\n",
    "    def special_creation_event_label(self, operation, value, entity_mapper):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def special_repository_label(self, operation, value, entity_mapper, field_name = \"repository_label\"):\n",
    "        # For each non-empty, non-NA, non-NS row: (1) generate L entity with label = value in this col., status = “approved”, entity logical type = “definite”, label language = value in the next col. (repository_label_language); (2) append to this L entity a metaprop (has) - C0565 “class” - C2646 “archive or library”, and (3) under the O entity representing the row (i.e. the physical manuscript), add a metaprop which will relate this O to this L entity (repository) through the relation: O - (has) - C2645 “repository” - L in this col.\n",
    "\n",
    "        origin = f\"Making location '{value}' from \" + entity_mapper.data_row['legacyId'] + f\" by field {field_name}.\"\n",
    "\n",
    "        if value != \"\" and value!=\"NA\" and value!=\"N/A\" and value!=\"NS\":  # check value\n",
    "            # check whether location exists with the same label\n",
    "            if tables[\"locations\"][tables[\"locations\"]['value']==value].empty: # NO\n",
    "                lentity = entity_mapper.make_lentity(label=value, legacyId=\"L_from_\"+entity_mapper.data_row['legacyId'], origin=origin)\n",
    "                lentity['language'] = entity_mapper.enum_mapper['language'][entity_mapper.data_row['repository_label_language']]\n",
    "            else: #YES\n",
    "                lentity = {}\n",
    "                lentity['id'] = tables[\"locations\"][tables[\"locations\"]['value']==value].iloc[0]['id']\n",
    "\n",
    "            # OLD metaprop way\n",
    "            #prop_type_id = entity_mapper.get_entity_id(\"C0565\", origin = origin)\n",
    "            #prop_value_id = entity_mapper.get_entity_id(\"C2646\", origin = origin)\n",
    "            #prop_object = entity_mapper.make_prop_object(prop_type_id, prop_value_id)\n",
    "            #lentity['props'].append(prop_object)\n",
    "\n",
    "            #(2) append to this L entity a RelationType.Classification leading to C2646 \"archive or library\"\n",
    "            entity_mapper.make_relation_classification_record(lentity['id'], entity_mapper.get_entity_id('C2646'))\n",
    "\n",
    "            # create audit record\n",
    "            entity_mapper.create_audit_record(entity_id=lentity['id'], object=lentity)\n",
    "\n",
    "            # let the manuscript O own the location\n",
    "            entity_mapper.hook_prop_object(prop_type = \"C2645\", input_value = lentity['id'], origin = origin)\n",
    "\n",
    "\n",
    "    # solves the method above\n",
    "    def special_repository_label_language(self, operation, value, entity_mapper):\n",
    "        pass\n",
    "\n",
    "    def special_reproduction_online_url(self, operation, value, entity_mapper, field_name = \"reproduction_online_url\"):\n",
    "        # If non-empty, non-NA, (1) generate an R entity with label \"Reproduction of \" + label of the MS (i.e. value in the B column, status = “approved”, label-language = “English”, url = the URL sitting under the hyperlink value in this cell, and (2) add metaprop to the O entity represented by this row: O - (has) - C1199 “digital reproduction” - the R entity here generated.\n",
    "\n",
    "        origin = f\"Making Resource entity '{value}' from \" + entity_mapper.data_row['legacyId'] + f\" by field {field_name}.\"\n",
    "\n",
    "        if value != \"\" and value!=\"NA\" and value!=\"N/A\" and value!=\"NS\":  # check value\n",
    "\n",
    "            if \"|\" in value:\n",
    "                data = value.split(\"|\")\n",
    "                label = data[0]\n",
    "                url = data[1]\n",
    "\n",
    "                if label == \"link\":\n",
    "                    label = \"Reproduction of \" + entity_mapper.data_row['label']\n",
    "\n",
    "            else:\n",
    "                url = \"\"\n",
    "                label = value\n",
    "                logger.warning(f\"Expected char | signaling url after label. Got just {value}. \"+origin)\n",
    "\n",
    "\n",
    "            tdf = tables['resources']\n",
    "            check_rentity = tdf[(tdf['label'] == label) & (tdf['dissinet_repository_url'] == url)]\n",
    "            if check_rentity.empty:\n",
    "                rentity = entity_mapper.make_rentity(label, url, origin=origin)\n",
    "                rentity['language'] = entity_mapper.enum_mapper['language']['English']\n",
    "            else:\n",
    "                rentity = {'id': check_rentity.iloc[0]['id']}\n",
    "\n",
    "            entity_mapper.hook_prop_object(prop_type = \"C1199\", input_value = rentity['id'], origin = origin)\n",
    "\n",
    "            # create audit record\n",
    "            entity_mapper.create_audit_record(entity_id=rentity['id'], object=rentity)\n",
    "\n",
    "\n",
    "class ResourceParser(Parser):\n",
    "\n",
    "    def __init__(self, name, header_df: pd.DataFrame, table_df: pd.DataFrame, keyword_row_id: int, logger: logger):\n",
    "        Parser.__init__(self, name, header_df, table_df, keyword_row_id, logger)\n",
    "\n",
    "\n",
    "class PersonParser(Parser):\n",
    "\n",
    "    def __init__(self, name, header_df: pd.DataFrame, table_df: pd.DataFrame, keyword_row_id: int, logger: logger):\n",
    "        Parser.__init__(self, name, header_df, table_df, keyword_row_id, logger)\n",
    "\n",
    "    # TODO implement\n",
    "    def special_introducers_id(self, operation, value, entity_mapper, field_name = \"introducers_id\"):\n",
    "        pass\n",
    "\n",
    "\n",
    "class EventParser(Parser):\n",
    "\n",
    "    def __init__(self, name, header_df: pd.DataFrame, table_df: pd.DataFrame, keyword_row_id: int, logger: logger):\n",
    "        Parser.__init__(self, name, header_df, table_df, keyword_row_id, logger)\n",
    "\n",
    "    def checkTerritoryExists(self,legacyId):\n",
    "        if \"-\" in legacyId:\n",
    "            if pd.DataFrame(additional_entities)[pd.DataFrame(additional_entities)['legacyId']== legacyId].empty:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        else: # main text\n",
    "            if tables[\"texts\"][tables[\"texts\"][\"legacyId\"]==legacyId].empty:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "\n",
    "    def special_subterritory_id(self, operation, value, entity_mapper, field_name = \"subterritory_id\"):\n",
    "        # Light yellow: document aka Territory metadata (not Event).\n",
    "        # please generate these T entities following Adam's script for parsing Robert's Sellan coding sheet IDs into a hierarchical T structure\n",
    "        # and put them under their proper root T through legacy IDs (first element of ID = root T ID).\n",
    "        # E.g. here, in Guglielmites, there is root, i.e. the whole T; then parts 1-4; and underneath them, the individual documents.\n",
    "        # Entity type = T; T label and label_language defined in the two respective cols.; entity_logical_type = definite; status = approved. Other metadata set as metaprops in the following yellow cols.\n",
    "\n",
    "        datarow = entity_mapper.data_row.copy()\n",
    "\n",
    "        # override of legacyId\n",
    "        # datarow['legacyId'] = value\n",
    "\n",
    "        # create territory proper\n",
    "        trt = self.EMP.make(entity_name=\"ITerritory\", data_row=datarow)\n",
    "        trt.make_alive(legacyId= datarow[\"subterritory_id\"], label=datarow['document_label'], label_language=datarow['document_label_language'], register_id_where=\"texts\")\n",
    "\n",
    "        trt.create_audit_record(entity_id=trt.entity['id'], object=trt.entity)\n",
    "\n",
    "        # propvalues\n",
    "        prop_value_fields = {\"state_of_conservation_id\":\"C3440\", \"participant_id\":\"C2863\", \"inquisitor_id\":\"C1541\", \"notary_id\":\"C1652\", \"witness_assessor_id\":\"C1540\"}\n",
    "\n",
    "        for pvf, concept in prop_value_fields.items():\n",
    "            if pvf not in entity_mapper.data_row:\n",
    "                continue\n",
    "            for item in self.itemize_valuestring_for_multiples(datarow[pvf]):\n",
    "                if item != \"\":\n",
    "                    trt.hook_prop_object(prop_type = concept, input_value = item)\n",
    "\n",
    "        # metaprop territory represent events\n",
    "        trt.hook_prop_object(prop_type = \"C2286\", input_value = entity_mapper.entity['id'])\n",
    "\n",
    "\n",
    "        # genre relation, \"genre_id\":\"C0335\",\n",
    "        for item in self.itemize_valuestring_for_multiples(datarow[\"genre_id\"]):\n",
    "            if item != \"\" in  item:\n",
    "                if \"C\" in item:\n",
    "                    trt.make_relation_record(\"Classification\",[trt.entity['id'], entity_mapper.get_entity_id(item)])\n",
    "                else:\n",
    "                    logger.error(f\"Creating territory through event generation:  did not get C entity for its classifications. Got '{item}'.\")\n",
    "\n",
    "        # audit record\n",
    "        trt.create_audit_record(entity_id=trt.entity['id'], object=trt.entity)\n",
    "\n",
    "        # check the ancestry #####################################################\n",
    "        t_parts = trt.entity['legacyId'].split(\"-\")\n",
    "        size = len(t_parts)\n",
    "\n",
    "\n",
    "        # save order\n",
    "        trt.entity['data']['parent']['order'] = int(t_parts[-1])\n",
    "\n",
    "        # if exists, connect, if doesnt, create, connect to root and connect the lowest part to the new middle\n",
    "        if size == 3:\n",
    "            checked_territory_lid = \"-\".join(t_parts[0:-1])\n",
    "            # logger.info(f\" Trying to go with {checked_territory_lid}.\")\n",
    "            if not self.checkTerritoryExists(checked_territory_lid): # territory does not exist\n",
    "                trt_new = self.EMP.make(entity_name=\"ITerritory\", data_row=datarow)\n",
    "                trt_new.make_alive(legacyId= checked_territory_lid, label=t_parts[1], label_language=datarow['document_label_language'], register_id_where=\"texts\")\n",
    "                parent_id = trt_new.entity['id']\n",
    "\n",
    "                checked_territory_lid = \"-\".join(t_parts[0:-2])\n",
    "                if self.checkTerritoryExists(checked_territory_lid):\n",
    "                    root_territory_id = entity_ids[\"texts\"][checked_territory_lid]\n",
    "                    trt_new.entity['data']['parent']['territoryId'] = root_territory_id\n",
    "                    trt_new.entity['data']['parent']['order'] = int(t_parts[-2])\n",
    "                else:\n",
    "                    logger.error(f\"The root territory {checked_territory_lid} does not exists.\")\n",
    "\n",
    "                # audit record\n",
    "                trt_new.create_audit_record(entity_id=trt_new.entity['id'], object=trt_new.entity)\n",
    "            else:\n",
    "                parent_id =  entity_ids['texts'][checked_territory_lid]\n",
    "\n",
    "            # territory does exist, connect the triggering territory to its parent\n",
    "            trt.entity['data']['parent']['territoryId'] = parent_id\n",
    "        elif size == 2:\n",
    "            # just connect it to the supposedly existing root\n",
    "            checked_territory_lid = \"-\".join(t_parts[0:1])\n",
    "            if self.checkTerritoryExists(checked_territory_lid):\n",
    "                root_territory_id = entity_ids[\"texts\"][checked_territory_lid]\n",
    "                trt.entity['data']['parent']['territoryId'] = root_territory_id\n",
    "                trt.entity['data']['parent']['order'] = int(t_parts[-1])\n",
    "            else:\n",
    "                logger.error(f\"The root territory {checked_territory_lid} does not exists.\")\n",
    "\n",
    "\n",
    "class LocationParser(Parser):\n",
    "\n",
    "    def __init__(self, name, header_df: pd.DataFrame, table_df: pd.DataFrame, keyword_row_id: int, logger: logger):\n",
    "        Parser.__init__(self, name, header_df, table_df, keyword_row_id, logger)\n",
    "\n",
    "    def special_modern_name(self, operation, value, entity_mapper, field_name = \"special_modern_name\"):\n",
    "        # \"For non-empty, generate new L entities from this col.\n",
    "        # 1) Attribute values: label = this col.; status = \"\"approved\"\", label_language = next col., entity_logical_type = same as col. D.\n",
    "        # 2) Create \"\"identical entity\"\" relation between the Latin entity captured by this row (\"\"id\"\") and this new entity.  Exact structure TBA (awaiting Adam) - until it is, create the relation as follows: append a metaprop to the original Latin location in \"\"id\"\", which will say: [\"\"id\"\" Location] - has - C0691 identical entity - [modern location created from modern_name].)\n",
    "        # 3) Attach \"\"class\"\" metaprop to this modern location from col. \"\"modern_name_class_id\"\" (described by the regular instructions).\"\n",
    "\n",
    "        origin = f\"Making Location entity '{value}' from \" + entity_mapper.data_row['legacyId'] + f\" by field {field_name}.\"\n",
    "        language = entity_mapper.data_row['modern_name_language']\n",
    "\n",
    "        if value != \"\" and value!=\"NA\" and value!=\"N/A\" and value!=\"NS\":  # check value\n",
    "            label = value\n",
    "            lentity = entity_mapper.make_lentity(label, origin=origin)\n",
    "            lentity['language'] = entity_mapper.enum_mapper['language'][language]\n",
    "\n",
    "\n",
    "            # create settlement prop OLD\n",
    "            #  pt C0565  value in modern_name_class_id  OLD\n",
    "            # prop_type = \"C0565\"\n",
    "            # prop_value = entity_mapper.data_row['modern_name_class_id'] # another C\n",
    "            #\n",
    "            # for item in self.itemize_valuestring_for_multiples(prop_value, origin=origin):\n",
    "            #     prop_type_id, prop_value_id = entity_mapper.prepare_prop_object_data(prop_type, item, origin=origin)\n",
    "            #\n",
    "            #     # make IProp object\n",
    "            #     prop_object = entity_mapper.make_prop_object(prop_type_id, prop_value_id)\n",
    "            #     # append prop to the location entity\n",
    "            #     lentity['props'].append(prop_object)\n",
    "\n",
    "            # make classification for C0565 job\n",
    "            # logger.info(f\"{entity_mapper.data_row}\")\n",
    "            class_value = entity_mapper.data_row['modern_name_class_id'] # another C\n",
    "            for item in self.itemize_valuestring_for_multiples(class_value, origin=origin):\n",
    "                entity_mapper.make_relation_record('Classification', [lentity['id'], entity_mapper.get_entity_id(item)])\n",
    "\n",
    "\n",
    "            # make identity relation\n",
    "            # {type: RelationType.Identification, entityIds: [Location 1 id, Location 2 id], certainty: [Certainty.Certain]},\n",
    "            # {type: \"I\", entityIds: [Location 1 id, Location 2 id], certainty: \"1\"},\n",
    "            entity_mapper.make_relation_identity_record(entity_mapper.entity['id'], lentity['id'], \"1\")\n",
    "\n",
    "            # create audit record\n",
    "            entity_mapper.create_audit_record(entity_id=lentity['id'], object=lentity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_records = [] # null relation records\n",
    "audits = [] # null audits records\n",
    "additional_entities = [] # null additional entities records\n",
    "# making empty table of additional entities\n",
    "tables['values'] = pd.DataFrame(columns=['id','value','origin'])\n",
    "tables['locations'] = pd.DataFrame(columns=['id','value','origin','legacyId'])\n",
    "tables['events'] = pd.DataFrame(columns=['id','value','origin','legacyId'])\n",
    "tables['props'] = pd.DataFrame(columns=['id','type','type_id','value','value_id','original_field','origin','entityId','legacyId'])\n",
    "tables['relations'] = pd.DataFrame(columns=['id','type','logic','certainty','entityIds','origin'])\n",
    "\n",
    "\n",
    "# nullifying the generated entities in resources\n",
    "# TODO\n",
    "\n",
    "logger.info(f\"Start\")\n",
    "#cp = ParseController(entity_list=['texts',\"concepts\"])\n",
    "\n",
    "# cp = ParseController(entity_list=[\"R0075_persons\",\"R0035_locations\",\"R0083_events\"])\n",
    "#cp = ParseController(entity_list=[\"R0035_locations\",\"R0083_events\"])\n",
    "#cp = ParseController(entity_list=[\"texts\"])\n",
    "\n",
    "cp = ParseController(entity_list=['texts','actions', 'manuscripts','resources','concepts','R0006_persons', 'R0007_locations','R0008_events',\"R0075_persons\",\"R0035_locations\",\"R0083_events\"])\n",
    "\n",
    "# cp = ParseController(entity_list=['texts','actions', 'manuscripts','resources','concepts','R0006_persons', 'R0007_locations','R0008_events',\"R0075_persons\",\"R0035_locations\",\"R0083_events\"], existing_parser=cp, reparse_entity_list=['R0035_locations'])\n",
    "\n",
    "# cp = ParseController(entity_list=['texts', \"concepts\", \"R0008_events\"])\n",
    "#cp = ParseController(entity_list=[ \"R0007_locations\"])\n",
    "# , 'R0006_persons', 'R0007_locations'\n",
    "# cp = ParseController(entity_list=['texts','manuscripts','resources','concepts','actions','R0006_persons', 'R0007_locations']) # , 'R0006_person \\as', 'R0007_locations'\n",
    "# cp = ParseController(entity_list=['R0006_persons', 'R0007_locations'])\n",
    "#cp = ParseController(entity_list=['R0008_events'])\n",
    "#cp = ParseController(entity_list=['actions','concepts'])\n",
    "cp.parse()\n",
    "logger.info(f\"End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables['resources']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Merging reciprocal relation records\n",
    "Antonym, PropertyReciprocal, SubjectActant1Reciprocal, Synonym, Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANT, PRR, SAR, SYN, REL\n",
    "def getE(eIds):\n",
    "    eids = eIds.copy() # has to be copy, otherwise it modifies original relation_records list-arrays\n",
    "    eids.sort()\n",
    "    return eids[0]+\"!\"+eids[1]\n",
    "\n",
    "rdf = pd.DataFrame(relation_records)\n",
    "rdf['eids'] = rdf['entityIds'].apply(lambda row:getE(row))\n",
    "removeRR  = rdf[rdf[['type','eids']].duplicated()].sort_values('eids')['id'].to_list()\n",
    "removeRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_records_cleaned = []\n",
    "for item in relation_records:\n",
    "    if item['id'] not in removeRR:\n",
    "        relation_records_cleaned.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(relation_records), len(relation_records_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for relations entityIds\n",
    "# a=0\n",
    "# for item in relation_records_cleaned:\n",
    "#     #if \"81e38bb2-6f84-4b30-acac-b6931929dbaf\" == item['id']:\n",
    "#     if \"7db804db-fa2d-4624-810d-b3440e2f4001\" in item['entityIds'][0] and item['type']==\"SCL\":\n",
    "#         a += 1\n",
    "#         print(item)\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%reload_ext line_profiler\n",
    "\n",
    "# def profile():\n",
    "#    cp.parsers['concepts'].parse_rows()\n",
    "\n",
    "#%lprun -f EntityMapper.get_entity_id profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %prun -s tottime profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %lprun?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables['events']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is this d72b2bee-47b4-4abc-8c88-793c72ab676d ?\n",
    "\n",
    "# {'parent': {'territoryId': 'eac54f75-822d-4c41-a995-c41ee4d1b8fc', 'order': 0}}\n",
    "\n",
    "pd.DataFrame(additional_entities)[pd.DataFrame(additional_entities)['id']==\"d98d04ac-850a-4274-a292-2210e6628afd\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Last checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(cp.parsers['actions'].js_objects[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"There are {len(additional_entities)} additionally created entities (e.g. values, resources ...).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Output the parsed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root data\n",
    "root_territory = \"\"\"\n",
    " {\n",
    "    \"id\": \"T0\",\n",
    "    \"class\": \"T\",\n",
    "    \"data\": { \"parent\": false },\n",
    "    \"label\": \"root\",\n",
    "    \"detail\": \"\",\n",
    "    \"language\": \"lat\",\n",
    "    \"notes\": [],\n",
    "    \"props\": []\n",
    "  }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connects outcomes from the individual parsers\n",
    "cp.load_json_objects()\n",
    "\n",
    "# save json object list of the parsed entities from google sheets in the text file\n",
    "with open('../datasets/all-test/new_entities.json', 'w', encoding='utf-8') as f:\n",
    "    #f.write(str(cp.parsers['texts'].js_objects))\n",
    "    json.dump(cp.js_objects, f)\n",
    "\n",
    "# save json object list of the \"newly created entities\" in the text file\n",
    "with open('../datasets/all-test/additional_entities.json', 'w', encoding='utf-8') as f:\n",
    "    #f.write(str(cp.parsers['texts'].js_objects))\n",
    "    json.dump(additional_entities, f)\n",
    "\n",
    "# read  entities.json\n",
    "with open('../datasets/all/entities.json','r') as f:\n",
    "    #entities_content = f.readlines()\n",
    "    entities_content = f.read().replace('\\n', '')\n",
    "\n",
    "# read  entities.json\n",
    "with open('../datasets/all-test/new_entities.json','r') as f:\n",
    "    #entities_content = f.readlines()\n",
    "    new_entities_content = f.read().replace('\\n', '')\n",
    "\n",
    "additional_entities_string = json.dumps(additional_entities)\n",
    "\n",
    "# write new and combine with old test data\n",
    "with open('../datasets/all-test/entities.json','w', encoding='utf-8') as f:\n",
    "    #merge_content = entities_content[0:-1] +  str(cp.parsers['texts'].js_objects)[1:]\n",
    "    merge_content = entities_content[0:-1] +\", \" + additional_entities_string[1:-1]+ \", \" + new_entities_content[1:]\n",
    "    f.write(str(merge_content))\n",
    "\n",
    "# write just the new parse data to the entities json.\n",
    "with open('../datasets/all-parsed/entities.json','w', encoding='utf-8') as f:\n",
    "    #merge_content = entities_content[0:-1] +  str(cp.parsers['texts'].js_objects)[1:]\n",
    "    merge_content =  \"[\" + root_territory + \",\" + additional_entities_string[1:-1]+ \", \" + new_entities_content[1:]\n",
    "    f.write(str(merge_content))\n",
    "\n",
    "\n",
    "# write relations\n",
    "with open('../datasets/all-parsed/relations.json','w', encoding='utf-8') as f:\n",
    "    json.dump(relation_records_cleaned, f)\n",
    "\n",
    "# write the audits json.\n",
    "with open('../datasets/all-parsed/audits.json','w', encoding='utf-8') as f:\n",
    "    json.dump(audits, f)\n",
    "\n",
    "\n",
    "logger.info(\"END\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(relation_records), len(relation_records_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log tables google sheet\n",
    "# https://docs.google.com/spreadsheets/d/1UpbJckLrRYTCz7wcEYhhcc-ihJxiUENx1unCCQqE4TE/edit#gid=0\n",
    "\n",
    "\n",
    "# d.open_gsheet(\"logtables\",\"https://docs.google.com/spreadsheets/d/1UpbJckLrRYTCz7wcEYhhcc-ihJxiUENx1unCCQqE4TE\")\n",
    "\n",
    "# d.write_df_to_gsheet(\"logtables\", \"values\", tables[\"values\"])\n",
    "# d.write_df_to_gsheet(\"logtables\", \"resources\", tables[\"resources\"])\n",
    "# d.write_df_to_gsheet(\"logtables\", \"locations\", tables[\"locations\"])\n",
    "# d.write_df_to_gsheet(\"logtables\", \"events\", tables[\"events\"])\n",
    "# d.write_df_to_gsheet(\"logtables\", \"props\", tables[\"props\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.open_gsheet(\"discouraged\",\"https://docs.google.com/spreadsheets/d/10x8TV2OmkcGt6zHw-3B-I3BhonETa0iQV424vCB9qSQ/edit#gid=0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discouraged_concepts = tables['concepts'][tables['concepts']['status']==\"discouraged\"][['legacyId','label']]\n",
    "discouraged_concepts_ids_list = discouraged_concepts['legacyId'].to_list()\n",
    "discouraged_concepts_ids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discouraged_concepts.set_index('legacyId').loc['C3324']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = [('table','legacyId','column_with_concept','discouraged_concept_id','discouraged_concept_label')]\n",
    "\n",
    "for t, tt in tables.items():\n",
    "    #df = tables['resources']\n",
    "    df = tables[t]\n",
    "    #mask = np.column_stack([df[col].str.contains(r\"C00\", na=False) for col in df])\n",
    "    mask = np.column_stack([df[col].isin(discouraged_concepts_ids_list) for col in df])\n",
    "    result = df.loc[mask.any(axis=1)]\n",
    "    if not result.empty:\n",
    "        #found[t] = []\n",
    "        print(t)\n",
    "        lids = result['legacyId'].to_list()\n",
    "        print(lids)\n",
    "        for l in lids:\n",
    "            target = df[df['legacyId'] == l]\n",
    "            for c in target.columns:\n",
    "                #print(target[c].iloc[0])\n",
    "                if c==\"legacyId\":\n",
    "                    continue\n",
    "                if t==\"concepts\":\n",
    "                    if target['status'].iloc[0] == \"discouraged\":\n",
    "                        continue\n",
    "                if target[c].iloc[0] in discouraged_concepts_ids_list:\n",
    "                    found.append((t,l,c, target[c].iloc[0],discouraged_concepts.set_index('legacyId').loc[target[c].iloc[0]]['label']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf = pd.DataFrame(found)\n",
    "new_header = rdf.iloc[0] #grab the first row for the header\n",
    "rdf = rdf[1:] #take the data less the header row\n",
    "rdf.columns = new_header\n",
    "rdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.write_df_to_gsheet(\"discouraged\", \"discouraged_concepts\", rdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask = np.column_stack([df[col].str.contains(r\"\\^\", na=False) for col in df])\n",
    "#df.loc[mask.any(axis=1)]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
