{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Parse development notebook\n",
    "\n",
    "### Notebook purpose\n",
    "This notebook is development space for python parse.ts replacement and upgrade.\n",
    "It reads specified google sheets and output actants.json file, which can be imported to inkVisitor RethinkDB.py\n",
    "\n",
    "The aggreated doc for dev: https://docs.google.com/document/d/1ga6R_9TWAQlXE9XqPE_qZ2S8S8aVW2A7n8SuYpSXnoI/edit#heading=h.q9ntf0ofam2u\n",
    "The holy schema: https://app.diagrams.net/#G1bKvqEKr6JzPWryVg-vYudQy_4KvuzHS9\n",
    "\n",
    "### JSon schemas for the actants\n",
    " * are created from *.ts files through typescript-jschema commandline utility\n",
    " * the schemas are trasformed as python Classes through warlock library\n",
    "\n",
    "\n",
    "### Principles of the parsing operations\n",
    " * There are google sheet dataset, which need to be transformed to json format according to inkVisior datamodel\n",
    " * inkVisitor holds datamodel in the code, the typescript classes (see /shared/types...), or the holy schema\n",
    " * DZ created in input table parsing instructions\n",
    "   * the first 5 rows contain instructions\n",
    "   * if the keywords contain ? character, they are ignored (it is work in progress from DZ)\n",
    "\n",
    "#### Parsing instructions\n",
    " * explicit set of keywords\n",
    "   * discard\n",
    "     * ...\n",
    "   * inside\n",
    "     * the value in the column should be straitforwadly made part of the entity object, the exists update_generic method or update_fieldName mehthods for fields with custom but generally applicable logic\n",
    "   * propvalue\n",
    "     * for making so called metastatemnt or property statment (old A0093 has relation)\n",
    "     * it sits in the entity \"props\" attribute, it has IProp class\n",
    "   * special\n",
    "     * fully custom method for the parsing behavior\n",
    "     *\n",
    "\n",
    "### Parsing process\n",
    " 1. Prepare data model entities classes (from typescript inkvisitor classes -> json schema ->  python classes)\n",
    " 2. Load and wrangle all input data\n",
    " 3. Process headers of the tables for parsing instructions\n",
    " 4. Process tables row by row, column by column\n",
    " 5. Save as json file (which can be imported to the inkVisitor RethinkDB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_tables = [\"texts\", \"manuscripts\", \"resources\", \"actions\" , \"concepts\"]\n",
    "\n",
    "#                   sheet_name,  code, header_in_row\n",
    "input_sheets = {\n",
    "    \"texts\" : (\"Texts\",\"13eVorFf7J9R8YzO7TmJRVLzIIwRJS737r7eFbH1boyE\", 5), #https://docs.google.com/spreadsheets/d/13eVorFf7J9R8YzO7TmJRVLzIIwRJS737r7eFbH1boyE/edit#gid=2056508047\n",
    "    \"manuscripts\" : (\"Manuscripts\", \"13eVorFf7J9R8YzO7TmJRVLzIIwRJS737r7eFbH1boyE\", 5),\n",
    "    \"resources\" : (\"Resources\", \"13eVorFf7J9R8YzO7TmJRVLzIIwRJS737r7eFbH1boyE\", 5),\n",
    "    \"actions\" :  (\"Statements\",\"1vzY6opQeR9hZVW6fmuZu2sgy_izF8vqGGhBQDxqT_eQ\", 5), # https://docs.google.com/spreadsheets/d/1vzY6opQeR9hZVW6fmuZu2sgy_izF8vqGGhBQDxqT_eQ/edit#gid=0\n",
    "    \"concepts\" : (\"Concepts\",\"1nSqnN6cjtdWK-y6iKZlJv4iGdhgtqkRPus8StVgExP4\", 5) # https://docs.google.com/spreadsheets/d/1nSqnN6cjtdWK-y6iKZlJv4iGdhgtqkRPus8StVgExP4/edit#gid=0\n",
    "}\n",
    "\n",
    "table_to_entity = {\n",
    "    \"texts\" : \"ITerritory\",\n",
    "    \"manuscripts\" : \"IObject\",\n",
    "    \"resources\" : \"IResource\",\n",
    "    \"actions\" :  \"IAction\",\n",
    "    \"concepts\" : \"IConcept\"\n",
    "}\n",
    "\n",
    "root_sheet_url = \"https://docs.google.com/spreadsheets/d/\"\n",
    "google_api_dotenv_path = \"../env/.env.googleapi\"  # contains google api specs for sheet access with Dator\n",
    "schema_path = '../schemas/' # path for dir with schemas\n",
    "json_schemas = {}  # holder for schemas, so they can be used for jsonschema validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# subprocess.run(\"python generate-json-schemas.py\", shell=True,capture_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\D\\anaconda3\\envs\\python38\\lib\\site-packages\\numpy\\__init__.py:148: UserWarning: mkl-service package failed to import, therefore Intel(R) MKL initialization ensuring its correct out-of-the box operation under condition when Gnu OpenMP had already been loaded by Python process is not assured. Please install mkl-service package, see http://github.com/IntelPython/mkl-service\n",
      "  from . import _distributor_init\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Unable to import required dependencies:\nnumpy: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.8 from \"C:\\Users\\D\\anaconda3\\envs\\python38\\python.exe\"\n  * The NumPy version is: \"1.21.2\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8780/2594950772.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mjsonschema\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdissinetpytools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdator\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python38\\lib\\site-packages\\dissinetpytools\\dator.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpygsheets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python38\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmissing_dependencies\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     raise ImportError(\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[1;34m\"Unable to import required dependencies:\\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_dependencies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     )\n",
      "\u001b[1;31mImportError\u001b[0m: Unable to import required dependencies:\nnumpy: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.8 from \"C:\\Users\\D\\anaconda3\\envs\\python38\\python.exe\"\n  * The NumPy version is: \"1.21.2\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.\n"
     ]
    }
   ],
   "source": [
    "import os, warlock, json\n",
    "from datetime import datetime\n",
    "from jsonschema import validate\n",
    "import dissinetpytools.dator as dator\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from shutil import copyfile\n",
    "\n",
    "import uuid\n",
    "\n",
    "def get_uuid_id():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "def is_valid_uuid(val):\n",
    "    try:\n",
    "        uuid.UUID(str(val))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "# type hinting\n",
    "from collections.abc import Sequence, Callable\n",
    "from typing import List, Dict, Tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 2022-03-17 10:44:31 : Google authentification start\n",
      "20 2022-03-17 10:44:31 : Google authentification end\n",
      "20 2022-03-17 10:44:31 : Dator initiation succesfull end\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(google_api_dotenv_path) # fills os.environ['GDRIVE_API_CREDENTIALS']\n",
    "d = dator.Dator(loglevel=10, print_log_online=True, cache=True, project_name=\"inkvisitor-import\") # expects 'GDRIVE_API_CREDENTIALS' in the global system variables (os.environ)\n",
    "d.google_authenticate(secretfilepath=\"DDDproject-cd7d53f52e07.json\")\n",
    "logger = d.logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-17 10:44:31,977 INFO Class IActant available.\n",
      "2022-03-17 10:44:31,979 INFO Class IAction available.\n",
      "2022-03-17 10:44:31,981 INFO Class IConcept available.\n",
      "2022-03-17 10:44:31,983 INFO Class IEntity available.\n",
      "2022-03-17 10:44:31,985 INFO Class ILabel available.\n",
      "2022-03-17 10:44:31,987 INFO Class IObject available.\n",
      "2022-03-17 10:44:31,988 INFO Class IProp available.\n",
      "2022-03-17 10:44:31,990 INFO Class IResource available.\n",
      "2022-03-17 10:44:31,993 INFO Class IStatement available.\n",
      "2022-03-17 10:44:31,994 INFO Class ITerritory available.\n",
      "2022-03-17 10:44:31,996 INFO Class IUser available.\n",
      "2022-03-17 10:44:31,998 INFO Class IValue available.\n",
      "2022-03-17 10:44:31,999 INFO There are 12 json classes available (IActant IAction IConcept IEntity ILabel IObject IProp IResource IStatement ITerritory IUser IValue).\n"
     ]
    }
   ],
   "source": [
    "# read all schemas inside and warlock them as globally available classes\n",
    "schema_filenames = os.listdir(schema_path)\n",
    "json_classes = {}\n",
    "for schema in schema_filenames:\n",
    "    name = schema.split(\".\")[0]\n",
    "    file_handler = open(schema_path + schema,\"r\")\n",
    "    schema_json = json.load(file_handler)\n",
    "    json_schemas[name] = schema_json\n",
    "    globals()[name] = warlock.model_factory(schema_json)\n",
    "    json_classes[name] = globals()[name]\n",
    "    logger.info(\"Class \" + name + \" available.\")\n",
    "\n",
    "logger.info(f\"There are {len(json_classes.keys())} json classes available ({' '.join(json_classes.keys())}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-17 10:44:32,038 INFO Class IAction validated.\n",
      "2022-03-17 10:44:32,043 INFO Class IConcept validated.\n",
      "2022-03-17 10:44:32,048 INFO Class IValue validated.\n",
      "2022-03-17 10:44:32,052 INFO Class IProp validated.\n",
      "2022-03-17 10:44:32,056 INFO Class IResource validated.\n",
      "2022-03-17 10:44:32,062 INFO Class IObject validated.\n",
      "2022-03-17 10:44:32,069 INFO Class IStatement validated.\n",
      "2022-03-17 10:44:32,073 INFO Class ITerritory validated.\n"
     ]
    }
   ],
   "source": [
    "# factory for making entity objects, contains defaults with \"prerequisities\"\n",
    "\n",
    "class InkVisitorJSONObjectFactory:\n",
    "\n",
    "    classes = json_classes\n",
    "\n",
    "    json_class_defaults = {\n",
    "        'IAction':{\n",
    "            'class':'A', 'id':'','legacyId':'', 'label':'', 'language':'', 'detail':'','data':{'entities':{'a1':[],'a2':[],'s':[]},'valencies':{'a1':'','a2':'','s':''}}, 'props':[], 'notes':[], 'status':'1', 'references':[]\n",
    "        },\n",
    "        'IConcept':{\n",
    "            'class':'C', 'id':'', 'legacyId':'','label':'', 'language':'', 'detail':'','data':{}, 'props':[], 'notes':[], 'status':'1', 'references':[]\n",
    "        },\n",
    "        'IValue':{\n",
    "            'class':'V', 'id':'', 'label':'', 'language':'', 'detail':'','data':{'logicalType':'4'}, 'props':[], 'notes':[], 'status':'1', 'references':[]\n",
    "        },\n",
    "        'IProp':{\n",
    "            'bundleEnd':False,'bundleStart':False, 'certainty':'1', 'children':[], 'elvl':'1',  'id':'', 'logic':'1', 'mood':[], 'moodvariant':'1', 'bundleOperator':'=', 'type': {'id':'','elvl':'1','logic':'1','partitivity':'1','virtuality':'1'},'value':{'id':'','elvl':'1', 'logic':'1', 'partitivity':'1', 'virtuality':'1'}\n",
    "        },\n",
    "        'IResource':{\n",
    "             'class':'R', 'id':'', 'label':'', 'language':'', 'detail':'','data':{'link':''}, 'props':[], 'notes':[], 'status':'1', 'references':[]\n",
    "        },\n",
    "        'IObject':{\n",
    "             'class':'O', 'id':'', 'label':'', 'language':'', 'detail':'','data':{'logicalType':'1'}, 'props':[], 'notes':[], 'status':'1', 'references':[]\n",
    "        },\n",
    "        'IStatement':{\n",
    "             'class':'S', 'id':'', 'label':'', 'language':'', 'detail':'','data':{'actants':[], 'actions':[], 'references':[],'tags':[],'territory': {'id':'','order':0}, 'text':''}, 'props':[], 'notes':[], 'status':'1', 'references':[]\n",
    "        },\n",
    "        'ITerritory':{\n",
    "            'class':'T', 'id':'', 'legacyId':'','label':'', 'language':'', 'detail':'','data':{'parent':{ \"id\": \"T0\", \"order\": 0 }}, 'props':[], 'notes':[], 'status':'1', 'references':[]\n",
    "        },\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def make(self, entity_name, override_object=None):\n",
    "        if override_object is None:\n",
    "            override_object = {}\n",
    "        object = type(self).json_class_defaults[entity_name]\n",
    "        object.update(override_object)\n",
    "        return type(self).classes[entity_name](deepcopy(object))\n",
    "\n",
    "    def validate_defaults(self):\n",
    "        for e in self.json_class_defaults:\n",
    "            test = self.make(e, self.json_class_defaults[e])\n",
    "            d.logger.info(f\"Class {e} validated.\")\n",
    "\n",
    "\n",
    "IOF = InkVisitorJSONObjectFactory()\n",
    "IOF.validate_defaults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load input datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-17 10:44:32,088 INFO Calling for texts with sheet_name Texts.\n",
      "2022-03-17 10:44:32,149 INFO Calling for manuscripts with sheet_name Manuscripts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 2022-03-17 10:44:32 : Loading dataset from pickle cache Texts\n",
      "20 2022-03-17 10:44:32 : Loading separated dataset header from pickle cache Texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-17 10:44:32,183 INFO Calling for resources with sheet_name Resources.\n",
      "2022-03-17 10:44:32,197 INFO Calling for actions with sheet_name Statements.\n",
      "2022-03-17 10:44:32,249 INFO Calling for concepts with sheet_name Concepts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 2022-03-17 10:44:32 : Loading dataset from pickle cache Manuscripts\n",
      "20 2022-03-17 10:44:32 : Loading separated dataset header from pickle cache Manuscripts\n",
      "20 2022-03-17 10:44:32 : Loading dataset from pickle cache Resources\n",
      "20 2022-03-17 10:44:32 : Loading separated dataset header from pickle cache Resources\n",
      "20 2022-03-17 10:44:32 : Loading dataset from pickle cache Statements\n",
      "20 2022-03-17 10:44:32 : Loading separated dataset header from pickle cache Statements\n",
      "20 2022-03-17 10:44:32 : Loading dataset from pickle cache Concepts\n",
      "20 2022-03-17 10:44:32 : Loading separated dataset header from pickle cache Concepts\n"
     ]
    }
   ],
   "source": [
    "# empty value unifier\n",
    "def unify_empty_value(df: pd.DataFrame, empty_values=None, unified_empty_value =''):\n",
    "    if empty_values is None:\n",
    "        empty_values = ['NA', \"#N/A\", \"#VALUE!\"]\n",
    "    for naner in empty_values:\n",
    "        df = df.replace(naner,unified_empty_value)\n",
    "    df. fillna(unified_empty_value, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# load all input tables\n",
    "tables = {}\n",
    "header_infos = {}\n",
    "entity_ids = {}\n",
    "for key, sheet in input_sheets.items():\n",
    "    logger.info(f\"Calling for {key} with sheet_name {sheet[0]}.\")\n",
    "    tables[key], header_infos[key] = d.load_df_from_gsheet(sheet[0],root_sheet_url + sheet[1], sheet[0], fromCache=True, header_in_row=sheet[2], clean=True, fillna=True, cleanByColumn=\"label\", parse_hyperlink_formulas=True)\n",
    "    tables[key] = unify_empty_value(tables[key])\n",
    "    header_infos[key] = unify_empty_value(header_infos[key])\n",
    "\n",
    "    # code for legacyId copy and uuid creation\n",
    "    tables[key]['legacyId'] = tables[key]['id'].copy()\n",
    "    # inform instructive header about the new column and what to do with it\n",
    "    header_infos[key]['legacyId'] = \"\"\n",
    "    header_infos[key].at[3,'legacyId'] = \"inside\"\n",
    "    tables[key]['id'] = tables[key].apply(lambda x: get_uuid_id(), axis=1)  # generate unique id for each row\n",
    "    # make id dictionaries (it is much faster to search for keys there in legacyId>id retrievals)\n",
    "    ed = tables[key][[\"legacyId\",\"id\"]].set_index(\"legacyId\")\n",
    "    entity_ids[key] = ed[\"id\"].to_dict()\n",
    "\n",
    "# making empty table of values\n",
    "tables['values'] = pd.DataFrame(columns=['id','value','origin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>detail</th>\n",
       "      <th>label_language</th>\n",
       "      <th>wordnet_lemma_id</th>\n",
       "      <th>wordnet_synset_id</th>\n",
       "      <th>instance_entity_type</th>\n",
       "      <th>DEPRECATED_supertype0</th>\n",
       "      <th>DEPRECATED_supertype1</th>\n",
       "      <th>...</th>\n",
       "      <th>related_entity_label</th>\n",
       "      <th>related_entity_detail</th>\n",
       "      <th>masterlists_column_name</th>\n",
       "      <th>masterlists_column_value</th>\n",
       "      <th>text_example</th>\n",
       "      <th>note</th>\n",
       "      <th>recommendation</th>\n",
       "      <th>DEPRECATED_label_english</th>\n",
       "      <th>parsing_rows_explained</th>\n",
       "      <th>legacyId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>WordNet lemma (URI)</td>\n",
       "      <td>Wordnet meaning (synset)</td>\n",
       "      <td>This was for automatic instantiation of deprec...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Discard, because the examples (incl. exact ref...</td>\n",
       "      <td>Save as one item of the multi-note.</td>\n",
       "      <td>Save as one item of the multi-note.</td>\n",
       "      <td></td>\n",
       "      <td>Comment - sometimes important to read for pars...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\"origin\" (for props).</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>note</td>\n",
       "      <td>note</td>\n",
       "      <td></td>\n",
       "      <td>Prop type (for props) or further detail (for i...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inside</td>\n",
       "      <td>inside</td>\n",
       "      <td>inside</td>\n",
       "      <td>inside</td>\n",
       "      <td>inside</td>\n",
       "      <td>inside?</td>\n",
       "      <td>inside?</td>\n",
       "      <td>discard</td>\n",
       "      <td>discard</td>\n",
       "      <td>discard</td>\n",
       "      <td>...</td>\n",
       "      <td>discard</td>\n",
       "      <td>discard</td>\n",
       "      <td>discard</td>\n",
       "      <td>discard</td>\n",
       "      <td>discard</td>\n",
       "      <td>inside</td>\n",
       "      <td>inside</td>\n",
       "      <td>discard</td>\n",
       "      <td>discard</td>\n",
       "      <td>inside</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   status      id   label  detail label_language     wordnet_lemma_id  \\\n",
       "0                                                 WordNet lemma (URI)   \n",
       "1                                                                       \n",
       "2                                                                       \n",
       "3  inside  inside  inside  inside         inside              inside?   \n",
       "\n",
       "          wordnet_synset_id  \\\n",
       "0  Wordnet meaning (synset)   \n",
       "1                             \n",
       "2                             \n",
       "3                   inside?   \n",
       "\n",
       "                                instance_entity_type DEPRECATED_supertype0  \\\n",
       "0  This was for automatic instantiation of deprec...                         \n",
       "1                                                                            \n",
       "2                                                                            \n",
       "3                                            discard               discard   \n",
       "\n",
       "  DEPRECATED_supertype1  ... related_entity_label related_entity_detail  \\\n",
       "0                        ...                                              \n",
       "1                        ...                                              \n",
       "2                        ...                                              \n",
       "3               discard  ...              discard               discard   \n",
       "\n",
       "  masterlists_column_name masterlists_column_value  \\\n",
       "0                                                    \n",
       "1                                                    \n",
       "2                                                    \n",
       "3                 discard                  discard   \n",
       "\n",
       "                                        text_example  \\\n",
       "0  Discard, because the examples (incl. exact ref...   \n",
       "1                                                      \n",
       "2                                                      \n",
       "3                                            discard   \n",
       "\n",
       "                                  note                       recommendation  \\\n",
       "0  Save as one item of the multi-note.  Save as one item of the multi-note.   \n",
       "1                                                                             \n",
       "2                                 note                                 note   \n",
       "3                               inside                               inside   \n",
       "\n",
       "  DEPRECATED_label_english                             parsing_rows_explained  \\\n",
       "0                           Comment - sometimes important to read for pars...   \n",
       "1                                                       \"origin\" (for props).   \n",
       "2                           Prop type (for props) or further detail (for i...   \n",
       "3                  discard                                            discard   \n",
       "\n",
       "  legacyId  \n",
       "0           \n",
       "1           \n",
       "2           \n",
       "3   inside  \n",
       "\n",
       "[4 rows x 61 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_infos['concepts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Declaration of controlling classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "additional_entities = []\n",
    "\n",
    "# for controlling entity and mapping of its fields\n",
    "class EntityMapper:\n",
    "\n",
    "    # simple inside values mapping from input_values in gsheets to inkVisitor enums\n",
    "    # field: { FROM  : TO }\n",
    "    enum_mapper = {'language': {\"English\":\"eng\",\"Latin\":\"lat\",\"Occitan\":\"oci\",\"Middle English\":\"enm\",\"Czech\":\"ces\",\"Italian\":\"ita\",\"French\":\"fra\",\"German\":\"deu\"},\"status\":{\"approved\":\"1\",\"pending\":\"0\",\"discouraged\":\"2\",\"warning\":\"3\"}}\n",
    "    # status  Pending = \"0\",   Approved = \"1\",  Discouraged = \"2\",  Warning = \"3\",\n",
    "    valid_entity_classes = ['A','C','E','O','R','T','P','G','S','L','NULL']\n",
    "\n",
    "\n",
    "    IOF = InkVisitorJSONObjectFactory()\n",
    "\n",
    "    def __init__(self, entity_type, logger = d.logger):\n",
    "        self.entity =  type(self).IOF.make(entity_type)\n",
    "        self.logger = logger\n",
    "\n",
    "\n",
    "    def get_entity_id(self,entity_string, origin = \"\"):\n",
    "        id = \"\"\n",
    "        # logger.info(f\"Getting entity string {entity_string} in {origin}\")\n",
    "        try:\n",
    "            if entity_string.startswith(\"C\") and entity_string[1:].isnumeric():\n",
    "                # id = tables['concepts'].loc[tables['concepts']['legacyId'] == entity_string,'id'].values[0]\n",
    "                id = entity_ids[\"concepts\"][entity_string]\n",
    "            elif entity_string.startswith(\"~V~\"):\n",
    "                ventity = self.make_ventity(entity_string, origin=origin)\n",
    "                id = ventity['id']\n",
    "            elif entity_string.startswith(\"M\") and entity_string[1:].isnumeric():\n",
    "                # id = tables['manuscripts'].loc[tables['manuscripts']['legacyId'] == entity_string,'id'].values[0]\n",
    "                id = entity_ids[\"manuscripts\"][entity_string]\n",
    "            elif entity_string.startswith(\"A\") and entity_string[1:].isnumeric():\n",
    "                # id = tables['actions'].loc[tables['actions']['legacyId'] == entity_string,'id'].values[0]\n",
    "                id = entity_ids[\"actions\"][entity_string]\n",
    "            elif entity_string.startswit(\"R\") and entity_string[1:].isnumeric():\n",
    "                # id = tables['resources'].loc[tables['resources']['legacyId'] == entity_string,'id'].values[0]\n",
    "                id = entity_ids[\"resources\"][entity_string]\n",
    "            elif is_valid_uuid(entity_string):\n",
    "                id = entity_string\n",
    "\n",
    "        except IndexError as E:\n",
    "            logger.info(f\"Cannot get entity id from entity string {entity_string} in {origin}. {E}\")\n",
    "\n",
    "        if id != \"\" and isinstance(id, str):\n",
    "            # logger.info(f\"Got entity id {id} from entity string {entity_string} in {origin}\")\n",
    "            return id\n",
    "        else:\n",
    "            logger.error(f\"Cannot get entity id from {entity_string}.\")\n",
    "            raise Exception(f\"Cannot get entity id from {entity_string}.\")\n",
    "\n",
    "\n",
    "    def make_ventity(self, value_string, origin=\"\"):\n",
    "        # logger.info(f\"Generating ventity from {value_string}.\")\n",
    "        # generate value entity object...\n",
    "        ventity = IOF.make('IValue')\n",
    "        ventity['id'] = get_uuid_id()\n",
    "        ventity['label'] = value_string.replace(\"~V~\",\"\")\n",
    "\n",
    "        # register ventity\n",
    "        tables['values'] = tables['values'].append({'id':ventity['id'] ,'value':ventity['label'],\"origin\":\"\"},ignore_index=True )\n",
    "        additional_entities.append(ventity)\n",
    "\n",
    "        # logger.info(f\"Ventity id={ventity['id']} generated.\")\n",
    "\n",
    "        return ventity\n",
    "\n",
    "    def make_rentity(self, label, url = \"\", origin=\"\"):\n",
    "        # logger.info(f\"Generating rentity from {value_string}.\")\n",
    "        # generate resource entity object...\n",
    "        rentity = IOF.make('IResource')\n",
    "        rentity['id'] = get_uuid_id()\n",
    "        rentity['label'] = label\n",
    "        rentity['data']['link'] = url\n",
    "\n",
    "        # register rentity\n",
    "        tables['resources'] = tables['resources'].append({'id':rentity['id'] ,'value':rentity['label'],\"origin\":origin}, ignore_index=True )\n",
    "        additional_entities.append(rentity)\n",
    "\n",
    "        # logger.info(f\"Rentity id={rentity['id']} generated.\")\n",
    "        return rentity\n",
    "\n",
    "    # interprets prop_type (should be always concept (?)) and input_value (should be concept or value string)\n",
    "    # get ids of the prop_type and prop_value (possibly creates and register values object)\n",
    "    # make iProp object\n",
    "    # puts iProp object into the entity props property\n",
    "    def hook_prop_object(self, prop_type, input_value, prop_source=\"\",  origin=\"\"):\n",
    "        # allowed entities in type\n",
    "        assert \"C\" in prop_type[0], f\"Prop type unknown, C string entity expected? {prop_type}, {origin}\"\n",
    "\n",
    "        # allowed entities in input\n",
    "        allowed_strict_entities = ['C','M'] # should be followed by numbers\n",
    "        allowed_free_string_entities = ['~V~']\n",
    "\n",
    "        # checking input_value\n",
    "        if any(input_value.startswith(c)for c in allowed_strict_entities):\n",
    "            # check for numbers\n",
    "            if not input_value[1:].isnumeric():\n",
    "                input_value = \"~V~\"+input_value\n",
    "        elif not any(input_value.startswith(c)for c in allowed_free_string_entities):\n",
    "            input_value = \"~V~\"+input_value\n",
    "\n",
    "        # TODO this is useless now, because the line above will make ~V~ from everything uknown\n",
    "        assert any(input_value.startswith(c)  for c in allowed_strict_entities) or any(input_value.startswith(c)  for c in allowed_free_string_entities), f\"Prop value unknown, C string or V string entity expected? {input_value}, {origin}\"\n",
    "\n",
    "        prop_type_id = self.get_entity_id(prop_type, origin=origin)\n",
    "        prop_value_id = self.get_entity_id(input_value, origin=origin)\n",
    "        # logger.info(f\"{prop_type_id}, {prop_value_id}\")\n",
    "\n",
    "        # make IProp object\n",
    "        prop_object = IOF.make('IProp')\n",
    "        prop_object['id'] = get_uuid_id()\n",
    "        prop_object['type']['id'] = prop_type_id\n",
    "        prop_object['value']['id'] = prop_value_id\n",
    "\n",
    "\n",
    "        if prop_source !=\"\": # means propvalue_2nd\n",
    "            self.hook_2ndprop_into_props(prop_object,prop_source)\n",
    "            pass\n",
    "        else:\n",
    "            # hook directly into the entity object\n",
    "            self.hook_prop_into_props(prop_object)\n",
    "\n",
    "\n",
    "    def hook_prop_into_props(self,prop_object):\n",
    "        self.entity['props'].append(prop_object)\n",
    "\n",
    "    def hook_2ndprop_into_props(self,prop_object,fst_prop_identification):  # identification by concept id\n",
    "        key = 0\n",
    "        keyId = self.get_entity_id(fst_prop_identification)\n",
    "        assert len(keyId)>0, f\"Cannot recognize entity id from {fst_prop_identification}\"\n",
    "\n",
    "        # count, value in enumerate(values)\n",
    "        for count, po in enumerate(self.entity['props']):\n",
    "            if po['type']['id'] == keyId and len(po['children']) == 0:  # I am counting on the fact, that if there relations from multiples, they are processed in the specific right order\n",
    "               po['children'].append(prop_object)\n",
    "\n",
    "    # method invoker for the INSIDE operation with concrete fields\n",
    "    def update_inside_field(self, field_name, input_value, origin= \"\"):\n",
    "        if input_value != '':\n",
    "\n",
    "            if (\"#\" in input_value or \"~\" in input_value) and field_name!= \"note\" and \"https://docs.\" not in input_value:\n",
    "                self.logger.info(f\"ALERT # or ~ in the input value {input_value}\")\n",
    "\n",
    "            update_operation = \"update_\" + field_name\n",
    "            update_func = getattr(self, update_operation, self.update_generic)\n",
    "            update_func(field_name, input_value, origin)\n",
    "        else:\n",
    "            raise Exception(f\"Trying to update {field_name} with empty input value.\")\n",
    "\n",
    "    #########################################################################################################\n",
    "    # the naming of procedures corresponds to the name of the input_table fields,  used for inside operations\n",
    "\n",
    "    def update_label_language(self, field_name, input_value, origin = \"\"):\n",
    "        if input_value in type(self).enum_mapper['language']:\n",
    "            self.entity['language'] = type(self).enum_mapper['language'][input_value]\n",
    "        else:\n",
    "            self.logger.error(f\"Unable to set language in {origin}.\")\n",
    "            self.entity['language'] = input_value # will raise error\n",
    "\n",
    "    def update_status(self, field_name, input_value, origin = \"\"):\n",
    "        if input_value in type(self).enum_mapper['status']:\n",
    "            self.entity['status'] = type(self).enum_mapper['status'][input_value]\n",
    "        else:\n",
    "            self.logger.error(f\"Unable to set status in {origin}.\")\n",
    "            self.entity['status'] = input_value # will raise error\n",
    "\n",
    "    def update_note(self, field_name, input_value, origin = \"\"):\n",
    "        #self.logger.info(f\"Updating note with {input_value}.\")\n",
    "        if \"#\" in input_value:\n",
    "            values = [v.strip() for v in input_value.split(\"#\")]\n",
    "            for v in values:\n",
    "                self.entity['notes'].append(v)\n",
    "        else:\n",
    "            self.entity['notes'].append(input_value)\n",
    "\n",
    "    def update_id(self, field_name, input_value, origin = \"\"):\n",
    "        # self.entity['id'] = input_value\n",
    "        self.entity['id'] = input_value\n",
    "\n",
    "    def update_legacyId(self, field_name, input_value, origin = \"\"):\n",
    "        # logger.info(f\"Trying to set legacyId {type(input_value)}:'{input_value}' {origin}.\")\n",
    "        self.entity['legacyId'] = input_value\n",
    "\n",
    "    def update_label(self, field_name, input_value, origin = \"\"):\n",
    "        self.entity['label'] = input_value\n",
    "\n",
    "    def update_wordnet_lemma_id(self, field_name, input_value, origin = \"\"):\n",
    "        # self.logger.info(f\" wordnet_lemma_id NOT IMPLEMENTED \")\n",
    "        pass\n",
    "\n",
    "    def update_wordnet_synset_id(self, field_name, input_value, origin = \"\"):\n",
    "        # self.logger.info(f\" wordnet_synset_id NOT IMPLEMENTED \")\n",
    "        pass\n",
    "\n",
    "    def update_generic(self, field_name, input_value, origin = \"\"):\n",
    "        self.entity[field_name] = input_value\n",
    "\n",
    "\n",
    "\n",
    "class TerritoryEntityMapper(EntityMapper):\n",
    "    def __init__(self,entity_type, logger = d.logger):\n",
    "        EntityMapper.__init__(self,entity_type, logger)\n",
    "\n",
    "class ConceptEntityMapper(EntityMapper):\n",
    "    def __init__(self,entity_type, logger = d.logger):\n",
    "        EntityMapper.__init__(self,entity_type, logger)\n",
    "\n",
    "class ActionEntityMapper(EntityMapper):\n",
    "    def __init__(self,entity_type, logger = d.logger):\n",
    "        EntityMapper.__init__(self,entity_type, logger)\n",
    "\n",
    "    def update_subject_entity_type(self, operation, value, entity_mapper):\n",
    "        # self.logger.info(f\"AP custom field: subject_entity_type\")\n",
    "        entities = [e.strip() for e in value.split(\"|\")]\n",
    "        for e in entities:\n",
    "            if e in self.valid_entity_classes:\n",
    "                self.entity['data']['entities']['s'].append(e)\n",
    "            elif e == \"*\":\n",
    "                self.entity['data']['entities']['s'] = self.valid_entity_classes\n",
    "            else:\n",
    "                logger.error(f\"Non valid entity processed: {e} while AP.update_subject_entity_type().\")\n",
    "\n",
    "    def update_subject_valency(self, operation, value, entity_mapper):\n",
    "        # self.logger.info(f\"AP custom field: subject_valency\")\n",
    "        self.entity['data']['valencies']['s'] = value\n",
    "\n",
    "    def update_actant1_entity_type(self, operation, value, entity_mapper):\n",
    "        # self.logger.info(f\"AP custom field: actant1_entity_type\")\n",
    "        entities = [e.strip() for e in value.split(\"|\")]\n",
    "        for e in entities:\n",
    "            if e in self.valid_entity_classes:\n",
    "                self.entity['data']['entities']['a1'].append(e)\n",
    "            elif e == \"*\":\n",
    "                self.entity['data']['entities']['a1'] = self.valid_entity_classes\n",
    "            else:\n",
    "                logger.error(f\"Non valid entity processed: {e} while AP.update_actant1_entity_type().\")\n",
    "\n",
    "    def update_actant2_entity_type(self, operation, value, entity_mapper):\n",
    "        # self.logger.info(f\"AP custom field: actant2_entity_type\")\n",
    "        entities = [e.strip() for e in value.split(\"|\")]\n",
    "        for e in entities:\n",
    "            if e in self.valid_entity_classes:\n",
    "                self.entity['data']['entities']['a2'].append(e)\n",
    "            elif e == \"*\":\n",
    "                self.entity['data']['entities']['a2'] = self.valid_entity_classes\n",
    "            else:\n",
    "                logger.error(f\"Non valid entity processed: {e} while AP.update_actant2_entity_type().\")\n",
    "\n",
    "    def update_actant1_valency(self, operation, value, entity_mapper):\n",
    "        # self.logger.info(f\"AP custom field: actant1_valency\")\n",
    "        self.entity['data']['valencies']['a1'] = value\n",
    "\n",
    "    def update_actant2_valency(self, operation, value, entity_mapper):\n",
    "        # self.logger.info(f\"AP custom field: actant1_valency\")\n",
    "        self.entity['data']['valencies']['a2'] = value\n",
    "\n",
    "\n",
    "class ResourceEntityMapper(EntityMapper):\n",
    "    def __init__(self,entity_type, logger = d.logger):\n",
    "        EntityMapper.__init__(self,entity_type, logger)\n",
    "\n",
    "    def update_url(self, operation, value, entity_mapper):\n",
    "        # self.logger.info(f\"AP custom field: subject_entity_type\")\n",
    "        self.entity['data']['link'] = value\n",
    "\n",
    "\n",
    "class ObjectEntityMapper(EntityMapper):\n",
    "    def __init__(self,entity_type, logger = d.logger):\n",
    "        EntityMapper.__init__(self,entity_type, logger)\n",
    "\n",
    "\n",
    "class EntityMapperFactory:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def make(self,entity_name):\n",
    "        if 'ITerritory' == entity_name:\n",
    "            return TerritoryEntityMapper(entity_name)\n",
    "        elif 'IAction' == entity_name:\n",
    "            return ActionEntityMapper(entity_name)\n",
    "        elif 'IConcept' == entity_name:\n",
    "            return ConceptEntityMapper(entity_name)\n",
    "        elif 'IResource' == entity_name:\n",
    "            return ResourceEntityMapper(entity_name)\n",
    "        elif 'IObject' == entity_name:\n",
    "            return ObjectEntityMapper(entity_name)\n",
    "\n",
    "        else:\n",
    "            logger.warning(f\"Unrecognized entity class in entity mapper. Is this right?\")\n",
    "            return EntityMapper(entity_name)\n",
    "\n",
    "\n",
    "# CONTROL CLASS\n",
    "class ParseController():\n",
    "    def __init__(self, entity_list: [], keyword_row_id = 3,  logger = d.logger):\n",
    "        self.entity_list = entity_list\n",
    "        self.logger = logger\n",
    "        self.parsers = {}\n",
    "        self.js_objects = []\n",
    "\n",
    "        for e in self.entity_list:\n",
    "            if 'texts' in e:\n",
    "                self.parsers[e] = TextParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "            elif 'actions' in e:\n",
    "                self.parsers[e] = ActionParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "            elif 'concepts' in e:\n",
    "                self.parsers[e] = ConceptParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "            elif 'resources' in e:\n",
    "                self.parsers[e] = ResourceParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "            elif 'manuscripts' in e:\n",
    "                self.parsers[e] = ManuscriptParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "            else:\n",
    "                self.logger.warning(f\"Coming to basic Parser entity - strange '{e}' {type(e)}.\")\n",
    "                self.parsers[e] = Parser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "\n",
    "    def load_json_objects(self):\n",
    "        for name, p in self.parsers.items():\n",
    "            self.js_objects = self.js_objects + p.js_objects\n",
    "\n",
    "    def parse(self):\n",
    "        for name, p in self.parsers.items():\n",
    "            p.parse_rows()\n",
    "\n",
    "\n",
    "# WORKER CLASS\n",
    "class Parser():\n",
    "    EMP = EntityMapperFactory()\n",
    "\n",
    "    def __init__(self, name, header_df: pd.DataFrame, table_df: pd.DataFrame, keyword_row_id: int, logger: logger):\n",
    "        self.name = name\n",
    "        self.logname = name.upper()\n",
    "        self.input_header_df = header_df\n",
    "        self.input_table_df = table_df\n",
    "        self.prepared_table = pd.DataFrame()\n",
    "        self.keyword_row_id =  keyword_row_id\n",
    "        self.columns = self.input_header_df.columns.tolist()\n",
    "\n",
    "        self.parsing_instruction = {}\n",
    "        self.oper_columns = {'discard':[],'inside':[],'special':[],'unknown':[],'propvalue':[],'propvalue_2nd':[]}\n",
    "        self.logger = logger\n",
    "\n",
    "        # parsed json data holder\n",
    "        self.js_objects = []\n",
    "\n",
    "        # RUN\n",
    "        self.process_header_instructions()\n",
    "        self.prepare_input_table()\n",
    "\n",
    "    # \"parsing\" instructions\n",
    "    def process_header_instructions(self) -> (pd.DataFrame, pd.DataFrame):\n",
    "        keyword_row = self.input_header_df.iloc[self.keyword_row_id]\n",
    "        prop_type_row = self.input_header_df.iloc[self.keyword_row_id - 1]\n",
    "        source_node_row = self.input_header_df.iloc[self.keyword_row_id - 2]\n",
    "\n",
    "        log_uncertain_instructions = []\n",
    "\n",
    "        for c in self.columns:\n",
    "            instruction_candidate = str(keyword_row.at[c]).strip()\n",
    "            prop_type_candidate = str(prop_type_row.at[c]).strip()\n",
    "            source_node_candidate = str(source_node_row.at[c]).strip()\n",
    "\n",
    "            if c == '':\n",
    "                self.logger.error(f\"{self.logname} There is empty column in the dataset.\")\n",
    "                raise Exception(f\"{self.logname} There is empty column in the dataset.\")\n",
    "\n",
    "            if \"?\" in instruction_candidate or \"?\" in prop_type_candidate or \"?\" in source_node_candidate:\n",
    "                log_uncertain_instructions.append(f\"{c.upper()}:{instruction_candidate},{prop_type_candidate},{source_node_candidate}\")\n",
    "                instruction  = {'operation':'discard', 'target': None}\n",
    "                self.oper_columns['discard'].append(c)\n",
    "\n",
    "            if 'discard' in instruction_candidate:\n",
    "                instruction  = {'operation':'discard', 'target': None}\n",
    "                self.oper_columns['discard'].append(c)\n",
    "\n",
    "            elif 'propvalue' ==  instruction_candidate:\n",
    "                prop_type = prop_type_candidate\n",
    "                source_node = source_node_candidate\n",
    "                if \"?\" in prop_type or \"?\"  in source_node:\n",
    "                    instruction = {'operation':'unknown', 'target': None}\n",
    "                    self.oper_columns['unknown'].append(c)\n",
    "                else:\n",
    "                    instruction  = {'operation':'propvalue', 'type': prop_type, 'source':source_node} # source can be ignored, because the iProp object is sitting inside of it\n",
    "                    self.oper_columns['propvalue'].append(c)\n",
    "\n",
    "            elif 'propvalue_2nd' in instruction_candidate:\n",
    "                prop_type = prop_type_candidate\n",
    "                source_node = source_node_candidate\n",
    "                if \"?\" in prop_type or \"?\"  in source_node:\n",
    "                    instruction = {'operation':'unknown', 'target': None}\n",
    "                    self.oper_columns['unknown'].append(c)\n",
    "                else:\n",
    "                    instruction  = {'operation':'propvalue_2nd', 'type': prop_type, 'source':source_node} # source can NOT be ignored, it signals which existing iProp object will hold this iProp object\n",
    "                    self.oper_columns['propvalue_2nd'].append(c)\n",
    "\n",
    "            elif 'special' in instruction_candidate:\n",
    "                # looks for custom functions registered by column name\n",
    "                prop_type = prop_type_candidate\n",
    "                source_node = source_node_candidate\n",
    "                instruction  = {'operation':'special', 'type': prop_type, 'source':source_node}\n",
    "                self.oper_columns['special'].append(c)\n",
    "\n",
    "            elif 'inside' in instruction_candidate:\n",
    "                if \"?\" in c:\n",
    "                    instruction = {'operation':'unknown', 'target': None}\n",
    "                    self.oper_columns['unknown'].append(c)\n",
    "                else:\n",
    "                    instruction  = {'operation':'inside', 'target': None}\n",
    "                    if len(prop_type_candidate) > 0:\n",
    "                        instruction  = {'operation':'inside', 'target': prop_type_candidate}\n",
    "\n",
    "                    self.oper_columns['inside'].append(c)\n",
    "\n",
    "            else:\n",
    "                instruction = {'operation':'unknown', 'target': None}\n",
    "                self.oper_columns['unknown'].append(c)\n",
    "            self.parsing_instruction[c] = instruction\n",
    "\n",
    "        self.logger.info(f\"{self.logname} Uncertain parsing instructions in {len(log_uncertain_instructions)} columns: \" + \" \".join(log_uncertain_instructions) + \".\")\n",
    "        return self.parsing_instruction\n",
    "\n",
    "    def prepare_input_table(self):\n",
    "        ip = self.input_table_df.copy()\n",
    "\n",
    "        # discard  columns with discard and unknown operations\n",
    "        ip.drop(columns=self.oper_columns['discard']+self.oper_columns['unknown'], inplace=True)\n",
    "\n",
    "        self.logger.info(f\"{self.logname} {len(self.oper_columns['discard']+self.oper_columns['unknown'])} columns have been dropped (discard:{len(self.oper_columns['discard'])}, unknown:{len(self.oper_columns['unknown'])}). Table now has {len(ip.columns)} columns, inside:{len(self.oper_columns['inside'])},propvalue:{len(self.oper_columns['propvalue'])}, special:{len(self.oper_columns['special'])}. Originally {self.input_table_df.shape[1]} columns.\")\n",
    "\n",
    "        self.prepared_table = ip\n",
    "\n",
    "\n",
    "    def prepare_property(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def make_row_object(self):\n",
    "        class_name = table_to_entity[self.name]\n",
    "        return type(self).EMP.make(class_name)\n",
    "\n",
    "    def itemize_valuestring_for_multiples(self, value_with_multiples, origin=\"\") -> []:\n",
    "        values = []\n",
    "        if isinstance(value_with_multiples,str):\n",
    "            parsed_value = value_with_multiples.split('#')\n",
    "            values =  [item.strip() for item in parsed_value]\n",
    "        else:\n",
    "            raise Exception(f\"Expected value to be string. Got {type(value_with_multiples)}. {origin}\")\n",
    "        return values\n",
    "\n",
    "    def parse_rows(self):\n",
    "        self.logger.info(f\"Starting to parse {self.name}.\")\n",
    "\n",
    "        if  self.prepared_table['label'].isnull().any():\n",
    "            self.prepared_table  = self.prepared_table[self.prepared_table['label'].notna()]\n",
    "            self.logger.info(f\"Empty labels found in {self.name} table. (new entities added through parsing process). Adjusting.\")\n",
    "\n",
    "        for key, row in self.prepared_table.iterrows():\n",
    "            # logger.info(f\"{self.name} Processing row {key}\")\n",
    "            entity_mapper = self.make_row_object()\n",
    "            for name, value in row.items():\n",
    "                if name in self.parsing_instruction:\n",
    "                    operation = self.parsing_instruction[name]\n",
    "                else:\n",
    "                    continue # silently ignore unknown columns\n",
    "                # logger.info(f\"{self.name} Processing columns {name}, with value {value}. Op:{operation}\")\n",
    "\n",
    "                # force string\n",
    "                value = str(value)\n",
    "\n",
    "                if operation['operation'] == 'inside' and value != '' and '?' not in name:\n",
    "                    # logger.info(f\"{self.name} Processing columns {name}, with value {value}. Op:{operation}\")\n",
    "                    if operation['target']:\n",
    "                        name = operation['target']\n",
    "                    entity_mapper.update_inside_field(name,value,operation['operation'] +\">\"+ str(key)+\":\"+str(name)+\":\"+str(value))\n",
    "\n",
    "                if operation['operation'] == 'propvalue' and value != '':\n",
    "                    # logger.info(f\"{self.name} Processing columns {name}, with value {value}. Op:{operation}\")\n",
    "                    prop_type = operation['type']\n",
    "                    if prop_type == '' or 'C' not in prop_type:\n",
    "                        raise Exception(f\"Propvalue does not have prop type defined. C entity-string expected, got {key}, {name}, {value}\")\n",
    "                    for item in self.itemize_valuestring_for_multiples(value):\n",
    "                        entity_mapper.hook_prop_object(prop_type = prop_type, input_value = item, origin = operation['operation'] +\">\"+ str(key)+\":\"+str(name)+\":\"+str(value))\n",
    "\n",
    "                if operation['operation'] == 'propvalue_2nd' and value != '':\n",
    "                    # logger.info(f\"{self.name} Processing columns {name}, with value {value}. Op:{operation}\")\n",
    "                    prop_type = operation['type']\n",
    "                    prop_source_name = operation['source']  # header_name,  we need concept_id\n",
    "\n",
    "                    prop_source_id = self.parsing_instruction[prop_source_name]['type']\n",
    "                    assert 'C' in prop_source_id, f\"Trying to get to the concept of 1st level property to adress 2nd level property.\"\n",
    "\n",
    "                    if prop_type == '' or 'C' not in prop_type:\n",
    "                        raise Exception(f\"Propvalue does not have prop type defined. C entity-string expected, got {key}, {name}, {value}\")\n",
    "\n",
    "                    origin = operation['operation'] +\">\"+ str(key)+\":\"+str(name)+\":\"+str(value)\n",
    "\n",
    "                    for item in self.itemize_valuestring_for_multiples(value, origin=origin):\n",
    "                        entity_mapper.hook_prop_object(prop_type = prop_type, input_value = item, prop_source = prop_source_id, origin = origin)\n",
    "                    pass\n",
    "\n",
    "                if operation['operation'] == 'special' and value != \"\":\n",
    "                    # logger.info(f\"SPECIAL {operation} {value}\")\n",
    "                    func = getattr(self, 'special_'+name)\n",
    "                    func(operation, value, entity_mapper)\n",
    "\n",
    "            self.js_objects.append(entity_mapper.entity)\n",
    "            # break  DEV, checking parsing after first iteration\n",
    "\n",
    "\n",
    "class TextParser(Parser):\n",
    "\n",
    "    def __init__(self, name, header_df: pd.DataFrame, table_df: pd.DataFrame, keyword_row_id: int, logger: logger):\n",
    "        Parser.__init__(self, name, header_df, table_df, keyword_row_id, logger)\n",
    "\n",
    "    # special methods for fields, which needs fully individual processing\n",
    "    def special_edition_1(self, operation, value, entity_mapper, field_name=\"special_edition1\", ):\n",
    "        # Parse this col. as \"propvalue\" - but you need to generate the target entities since they do not exist. How to do it: for any value, create an R entity with \"label\" = textual value in this col., \"label language\" = English, \"status\" = \"approved\", and \"URL\" = the hyperlink in the formula sitting on the textual value in this col. As usual, ignore NS and NA values (exact match) - do not import anything if the value is NA.\n",
    "        # logger.info(f\"Special edition1 running ...{operation} {value}\")\n",
    "        prop_type = operation['type']\n",
    "\n",
    "\n",
    "        # make rentity\n",
    "        # logger.info(f\"Rentity making {value}\")\n",
    "        if \"|\" in value:\n",
    "            data = value.split(\"|\")\n",
    "            label = data[0]\n",
    "            url = data[1]\n",
    "        else:\n",
    "            url = \"\"\n",
    "            label = value\n",
    "            # logger.warning(f\"Expected char | signaling url after label. Got just {value}\")\n",
    "\n",
    "        rentity = entity_mapper.make_rentity(label, url, origin=field_name)\n",
    "\n",
    "        entity_mapper.hook_prop_object(prop_type = prop_type, input_value = rentity['id'], origin = operation['operation'] +\">\"+ \":\"+field_name + str(value))\n",
    "\n",
    "    def special_edition_2(self, operation, value, entity_mapper):\n",
    "        self.special_edition_1( operation, value, entity_mapper, field_name=\"edition_2\")\n",
    "\n",
    "    def special_edition_3(self, operation, value, entity_mapper):\n",
    "        self.special_edition_1( operation, value, entity_mapper, field_name=\"edition_3\")\n",
    "\n",
    "class ActionParser(Parser):\n",
    "\n",
    "    def __init__(self, name, header_df: pd.DataFrame, table_df: pd.DataFrame, keyword_row_id: int, logger: logger):\n",
    "        Parser.__init__(self, name, header_df, table_df, keyword_row_id, logger)\n",
    "\n",
    "\n",
    "class ConceptParser(Parser):\n",
    "\n",
    "    def __init__(self, name, header_df: pd.DataFrame, table_df: pd.DataFrame, keyword_row_id: int, logger: logger):\n",
    "        Parser.__init__(self, name, header_df, table_df, keyword_row_id, logger)\n",
    "\n",
    "class ManuscriptParser(Parser):\n",
    "\n",
    "    def __init__(self, name, header_df: pd.DataFrame, table_df: pd.DataFrame, keyword_row_id: int, logger: logger):\n",
    "        Parser.__init__(self, name, header_df, table_df, keyword_row_id, logger)\n",
    "\n",
    "class ResourceParser(Parser):\n",
    "\n",
    "    def __init__(self, name, header_df: pd.DataFrame, table_df: pd.DataFrame, keyword_row_id: int, logger: logger):\n",
    "        Parser.__init__(self, name, header_df, table_df, keyword_row_id, logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-17 10:44:32,520 INFO Start\n",
      "2022-03-17 10:44:32,526 INFO TEXTS Uncertain parsing instructions in 6 columns: TIMERELATION1_TARGET_ID:propvalue?,, TIMERELATION2_TYPE_CONCEPTIFIED_ID:proptype?,, TIMERELATION2_TARGET_ID:propvalue?,, TIMERELATION3_TYPE_CONCEPTIFIED_ID:proptype?,, TIMERELATION3_TARGET_ID:propvalue?,, TIMERELATION4_TARGET_ID:propvalue?,,.\n",
      "2022-03-17 10:44:32,528 INFO TEXTS 80 columns have been dropped (discard:72, unknown:8). Table now has 40 columns, inside:9,propvalue:14, special:3. Originally 114 columns.\n",
      "2022-03-17 10:44:32,531 INFO MANUSCRIPTS Uncertain parsing instructions in 6 columns: REPOSITORY_PLACE:??? this will be a superordinate location of repository,, MS_CREATION_START:_EARLIEST_POSSIBLE_DATE:???,, MS_CREATION_START:_LATEST_POSSIBLE_DATE:???,, MS_CREATION_END:_EARLIEST_POSSIBLE_DATE:???,, MS_CREATION_END:_LATEST_POSSIBLE_DATE:???,, REPRODUCTION_ONLINE_URL:???,,.\n",
      "2022-03-17 10:44:32,534 INFO MANUSCRIPTS 49 columns have been dropped (discard:23, unknown:26). Table now has 17 columns, inside:8,propvalue:9, special:0. Originally 60 columns.\n",
      "2022-03-17 10:44:32,536 INFO RESOURCES Uncertain parsing instructions in 0 columns: .\n",
      "2022-03-17 10:44:32,538 INFO RESOURCES 14 columns have been dropped (discard:14, unknown:0). Table now has 8 columns, inside:6,propvalue:2, special:0. Originally 22 columns.\n",
      "2022-03-17 10:44:32,541 INFO CONCEPTS Uncertain parsing instructions in 2 columns: WORDNET_LEMMA_ID:inside?,, WORDNET_SYNSET_ID:inside?,,.\n",
      "2022-03-17 10:44:32,544 INFO CONCEPTS 45 columns have been dropped (discard:44, unknown:1). Table now has 16 columns, inside:10,propvalue:8, special:0. Originally 61 columns.\n",
      "2022-03-17 10:44:32,555 INFO ACTIONS Uncertain parsing instructions in 0 columns: .\n",
      "2022-03-17 10:44:32,557 INFO ACTIONS 48 columns have been dropped (discard:48, unknown:0). Table now has 27 columns, inside:16,propvalue:11, special:0. Originally 75 columns.\n",
      "2022-03-17 10:44:32,557 INFO Starting to parse texts.\n",
      "2022-03-17 10:44:42,963 INFO Starting to parse manuscripts.\n",
      "2022-03-17 10:44:50,597 INFO Starting to parse resources.\n",
      "2022-03-17 10:44:53,691 INFO Starting to parse concepts.\n",
      "2022-03-17 10:45:59,058 INFO Starting to parse actions.\n",
      "2022-03-17 10:46:30,624 ERROR Non valid entity processed: \"hereticus\" while AP.update_actant1_entity_type().\n",
      "2022-03-17 10:46:30,625 ERROR Non valid entity processed: \"heretici\" while AP.update_actant1_entity_type().\n",
      "2022-03-17 10:46:31,478 INFO End\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Start\")\n",
    "cp = ParseController(entity_list=['texts','manuscripts','resources','concepts','actions'])\n",
    "cp.parse()\n",
    "logger.info(f\"End\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#%reload_ext line_profiler\n",
    "\n",
    "# def profile():\n",
    "#    cp.parsers['concepts'].parse_rows()\n",
    "\n",
    "#%lprun -f EntityMapper.get_entity_id profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %prun -s tottime profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %lprun?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Last checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"class\": \"O\", \"id\": \"bf102c79-b041-4482-8e3a-f693ceb7ff7a\", \"label\": \"Bologna, Biblioteca comunale dell\\u2019Archiginnasio, ms. B 1856\", \"language\": \"ita\", \"detail\": \"\", \"data\": {\"logicalType\": \"1\"}, \"props\": [{\"bundleEnd\": false, \"bundleStart\": false, \"certainty\": \"1\", \"children\": [], \"elvl\": \"1\", \"id\": \"c27f6b2c-2c07-4494-8f5e-2e6e8eec51a2\", \"logic\": \"1\", \"mood\": [], \"moodvariant\": \"1\", \"bundleOperator\": \"=\", \"type\": {\"id\": \"370507f6-a7d8-4c15-9db5-d01883080801\", \"elvl\": \"1\", \"logic\": \"1\", \"partitivity\": \"1\", \"virtuality\": \"1\"}, \"value\": {\"id\": \"c0301ef7-53e4-4c8e-a82b-e453dccd8075\", \"elvl\": \"1\", \"logic\": \"1\", \"partitivity\": \"1\", \"virtuality\": \"1\"}}, {\"bundleEnd\": false, \"bundleStart\": false, \"certainty\": \"1\", \"children\": [], \"elvl\": \"1\", \"id\": \"a0bef6f4-bf21-4c34-82be-6490e2df8afc\", \"logic\": \"1\", \"mood\": [], \"moodvariant\": \"1\", \"bundleOperator\": \"=\", \"type\": {\"id\": \"7420af9d-7c56-4146-8806-527433c4c91a\", \"elvl\": \"1\", \"logic\": \"1\", \"partitivity\": \"1\", \"virtuality\": \"1\"}, \"value\": {\"id\": \"ad12a271-5ccc-450d-8761-7f4ceaeb8a3b\", \"elvl\": \"1\", \"logic\": \"1\", \"partitivity\": \"1\", \"virtuality\": \"1\"}}, {\"bundleEnd\": false, \"bundleStart\": false, \"certainty\": \"1\", \"children\": [], \"elvl\": \"1\", \"id\": \"6c5a5905-6058-4a1e-965d-66172359c1de\", \"logic\": \"1\", \"mood\": [], \"moodvariant\": \"1\", \"bundleOperator\": \"=\", \"type\": {\"id\": \"6140b3e0-f8de-4965-9648-62c5985788fd\", \"elvl\": \"1\", \"logic\": \"1\", \"partitivity\": \"1\", \"virtuality\": \"1\"}, \"value\": {\"id\": \"965ebff4-e5cf-4ab7-94b3-1c49e2b8579f\", \"elvl\": \"1\", \"logic\": \"1\", \"partitivity\": \"1\", \"virtuality\": \"1\"}}, {\"bundleEnd\": false, \"bundleStart\": false, \"certainty\": \"1\", \"children\": [], \"elvl\": \"1\", \"id\": \"9e45a9dc-4a7a-4c5d-b486-5164fbdaaee0\", \"logic\": \"1\", \"mood\": [], \"moodvariant\": \"1\", \"bundleOperator\": \"=\", \"type\": {\"id\": \"02150dcb-d494-44e6-af47-e89aa831968b\", \"elvl\": \"1\", \"logic\": \"1\", \"partitivity\": \"1\", \"virtuality\": \"1\"}, \"value\": {\"id\": \"2a6a9a77-3c9e-4c3b-8b86-14437c634b0d\", \"elvl\": \"1\", \"logic\": \"1\", \"partitivity\": \"1\", \"virtuality\": \"1\"}}, {\"bundleEnd\": false, \"bundleStart\": false, \"certainty\": \"1\", \"children\": [], \"elvl\": \"1\", \"id\": \"88137614-48ea-4030-b54e-b8395076e0e1\", \"logic\": \"1\", \"mood\": [], \"moodvariant\": \"1\", \"bundleOperator\": \"=\", \"type\": {\"id\": \"26296391-82e6-43ec-8c36-88d98ee504de\", \"elvl\": \"1\", \"logic\": \"1\", \"partitivity\": \"1\", \"virtuality\": \"1\"}, \"value\": {\"id\": \"7710bcae-f841-4918-8bf0-28e9ddf917a3\", \"elvl\": \"1\", \"logic\": \"1\", \"partitivity\": \"1\", \"virtuality\": \"1\"}}, {\"bundleEnd\": false, \"bundleStart\": false, \"certainty\": \"1\", \"children\": [], \"elvl\": \"1\", \"id\": \"56f5d8f2-6630-476e-ad39-201c80514905\", \"logic\": \"1\", \"mood\": [], \"moodvariant\": \"1\", \"bundleOperator\": \"=\", \"type\": {\"id\": \"cdc9b58f-60c0-4057-89c2-8104b6c2b143\", \"elvl\": \"1\", \"logic\": \"1\", \"partitivity\": \"1\", \"virtuality\": \"1\"}, \"value\": {\"id\": \"3a95620e-27d3-4071-9b2f-9b1b0b88e49b\", \"elvl\": \"1\", \"logic\": \"1\", \"partitivity\": \"1\", \"virtuality\": \"1\"}}, {\"bundleEnd\": false, \"bundleStart\": false, \"certainty\": \"1\", \"children\": [], \"elvl\": \"1\", \"id\": \"712b6bb3-9ab7-4f07-8d61-33531c210d6a\", \"logic\": \"1\", \"mood\": [], \"moodvariant\": \"1\", \"bundleOperator\": \"=\", \"type\": {\"id\": \"5e55d195-f11e-41cc-ac9e-c1da192249cb\", \"elvl\": \"1\", \"logic\": \"1\", \"partitivity\": \"1\", \"virtuality\": \"1\"}, \"value\": {\"id\": \"d2069492-3af7-4939-ac38-19dee0369e58\", \"elvl\": \"1\", \"logic\": \"1\", \"partitivity\": \"1\", \"virtuality\": \"1\"}}], \"notes\": [\"The documents are contemporary versions, not copies, i.e. start is really 1291 and end 1310, at the time of the investigations.\"], \"status\": \"1\", \"references\": [], \"legacyId\": \"M106\"}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(cp.parsers['manuscripts'].js_objects[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-17 10:46:31,557 INFO There are 1158 additionally created entities (e.g. values, resources ...).\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"There are {len(additional_entities)} additionally created entities (e.g. values, resources ...).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cp.load_json_objects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Output the parsed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# root data\n",
    "root_territory = \"\"\"\n",
    " {\n",
    "    \"id\": \"T0\",\n",
    "    \"class\": \"T\",\n",
    "    \"data\": { \"parent\": false },\n",
    "    \"label\": \"root\",\n",
    "    \"detail\": \"\",\n",
    "    \"language\": \"lat\",\n",
    "    \"notes\": [],\n",
    "    \"props\": []\n",
    "  }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-17 10:46:32,692 INFO END\n"
     ]
    }
   ],
   "source": [
    "# save json object list of the parsed entities from google sheets in the text file\n",
    "with open('../datasets/all-test/new_entities.json', 'w', encoding='utf-8') as f:\n",
    "    #f.write(str(cp.parsers['texts'].js_objects))\n",
    "    json.dump(cp.js_objects, f)\n",
    "\n",
    "# save json object list of the \"newly created entities\" in the text file\n",
    "with open('../datasets/all-test/additional_entities.json', 'w', encoding='utf-8') as f:\n",
    "    #f.write(str(cp.parsers['texts'].js_objects))\n",
    "    json.dump(additional_entities, f)\n",
    "\n",
    "# read  entities.json\n",
    "with open('../datasets/all/entities.json','r') as f:\n",
    "    #entities_content = f.readlines()\n",
    "    entities_content = f.read().replace('\\n', '')\n",
    "\n",
    "# read  entities.json\n",
    "with open('../datasets/all-test/new_entities.json','r') as f:\n",
    "    #entities_content = f.readlines()\n",
    "    new_entities_content = f.read().replace('\\n', '')\n",
    "\n",
    "additional_entities_string = json.dumps(additional_entities)\n",
    "\n",
    "# write new and combine with old test data\n",
    "with open('../datasets/all-test/entities.json','w', encoding='utf-8') as f:\n",
    "    #merge_content = entities_content[0:-1] +  str(cp.parsers['texts'].js_objects)[1:]\n",
    "    merge_content = entities_content[0:-1] +\", \" + additional_entities_string[1:-1]+ \", \" + new_entities_content[1:]\n",
    "    f.write(str(merge_content))\n",
    "\n",
    "\n",
    "\n",
    "# write just the new parse data to the entities json.\n",
    "with open('../datasets/all-parsed/entities.json','w', encoding='utf-8') as f:\n",
    "    #merge_content = entities_content[0:-1] +  str(cp.parsers['texts'].js_objects)[1:]\n",
    "    merge_content =  \"[\" + root_territory + \",\" + additional_entities_string[1:-1]+ \", \" + new_entities_content[1:]\n",
    "    f.write(str(merge_content))\n",
    "\n",
    "logger.info(\"END\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
